{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_length = 50001\n",
    "time_per_sample = 0.01\n",
    "signal_time = np.linspace(num=sample_length,start = 0, stop = sample_length * time_per_sample )\n",
    "signal_amp = np.sin(signal_time*2*np.pi) + \\\n",
    "    np.sin(2+signal_time*1.7*np.pi)*0.5 + \\\n",
    "    np.sin(1+signal_time*2.2*np.pi) + \\\n",
    "    np.random.normal(size=sample_length)*0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "scatter",
         "x": [
          0,
          0.010000199999999999,
          0.020000399999999998,
          0.030000599999999995,
          0.040000799999999996,
          0.050001,
          0.06000119999999999,
          0.07000139999999999,
          0.08000159999999999,
          0.09000179999999999,
          0.100002,
          0.1100022,
          0.12000239999999998,
          0.1300026,
          0.14000279999999998,
          0.150003,
          0.16000319999999998,
          0.17000339999999997,
          0.18000359999999999,
          0.19000379999999997,
          0.200004,
          0.21000419999999997,
          0.2200044,
          0.23000459999999998,
          0.24000479999999996,
          0.250005,
          0.2600052,
          0.27000539999999995,
          0.28000559999999997,
          0.2900058,
          0.300006,
          0.31000619999999995,
          0.32000639999999997,
          0.3300066,
          0.34000679999999994,
          0.35000699999999996,
          0.36000719999999997,
          0.3700074,
          0.38000759999999995,
          0.39000779999999996,
          0.400008,
          0.41000819999999993,
          0.42000839999999995,
          0.43000859999999996,
          0.4400088,
          0.45000899999999994,
          0.46000919999999995,
          0.47000939999999997,
          0.4800095999999999,
          0.49000979999999994,
          0.50001,
          0.5100102,
          0.5200104,
          0.5300106,
          0.5400107999999999,
          0.5500109999999999,
          0.5600111999999999,
          0.5700114,
          0.5800116,
          0.5900118,
          0.600012,
          0.6100121999999999,
          0.6200123999999999,
          0.6300125999999999,
          0.6400127999999999,
          0.650013,
          0.6600132,
          0.6700134,
          0.6800135999999999,
          0.6900137999999999,
          0.7000139999999999,
          0.7100141999999999,
          0.7200143999999999,
          0.7300146,
          0.7400148,
          0.7500149999999999,
          0.7600151999999999,
          0.7700153999999999,
          0.7800155999999999,
          0.7900157999999999,
          0.800016,
          0.8100162,
          0.8200163999999999,
          0.8300165999999999,
          0.8400167999999999,
          0.8500169999999999,
          0.8600171999999999,
          0.8700173999999999,
          0.8800176,
          0.8900177999999999,
          0.9000179999999999,
          0.9100181999999999,
          0.9200183999999999,
          0.9300185999999999,
          0.9400187999999999,
          0.950019,
          0.9600191999999999,
          0.9700193999999999,
          0.9800195999999999,
          0.9900197999999999
         ],
         "y": [
          1.2904061747198659,
          1.3721451570896488,
          1.4830813460147056,
          1.5460726902950677,
          1.6284520655555037,
          1.6995525521693113,
          1.74781800172497,
          1.7220958161918305,
          1.8229461110464957,
          1.8619881936531288,
          1.8400301256290792,
          1.9009503487666801,
          1.863105614621909,
          1.8905083710603818,
          1.8650219855171717,
          1.8633904019386527,
          1.8303953462799623,
          1.800923267496681,
          1.7789319572775326,
          1.730680828930675,
          1.619730451287393,
          1.6153482963340446,
          1.571091391281538,
          1.4824826680840257,
          1.3922855447531204,
          1.317888893794123,
          1.229534125377882,
          1.1040121477615608,
          1.0423947208051123,
          0.9514310527132379,
          0.8073624309750395,
          0.7072861235454271,
          0.5530222120632857,
          0.425808892837975,
          0.31523943128977017,
          0.20548825381884275,
          0.10333033338382572,
          -0.031350115198373274,
          -0.1615014780668712,
          -0.24195778091388642,
          -0.4229236876642579,
          -0.551388046844491,
          -0.652503672441579,
          -0.8021848909205408,
          -0.8854316472837753,
          -0.9803064505745986,
          -1.0807471716177819,
          -1.2063958069233878,
          -1.2913815152156034,
          -1.373695468248517,
          -1.4730893754261052,
          -1.5416883221279662,
          -1.6185261342832578,
          -1.6816251833008593,
          -1.7483426685728587,
          -1.7846126367180093,
          -1.8585961949319496,
          -1.8740397294270845,
          -1.9166498085506656,
          -1.9068933698428499,
          -1.940586699842234,
          -1.9315952496754316,
          -1.9473290057766413,
          -1.9449617257317346,
          -1.9094189883280503,
          -1.8976558993607016,
          -1.8265699054095772,
          -1.7862466252194933,
          -1.7451158230903077,
          -1.6716399216848434,
          -1.6325876321786112,
          -1.5579727584389371,
          -1.5033298574540046,
          -1.4200995826437433,
          -1.360493029387998,
          -1.2127841246283435,
          -1.1435360321981802,
          -1.0584812686901863,
          -0.9494868573745819,
          -0.8385133734059343,
          -0.6963778702708578,
          -0.5943281811614144,
          -0.47905057582344274,
          -0.3590833160905554,
          -0.25402742669233247,
          -0.1584063674114831,
          0.009825792596144503,
          0.1186990680583482,
          0.22267193051376535,
          0.39099821009118335,
          0.47367211813382515,
          0.5813563623937186,
          0.714780627194707,
          0.8195561610996839,
          0.8705652501431911,
          1.0196729999322327,
          1.0842013384893507,
          1.2017480391512587,
          1.2670581387201654,
          1.3774367555440292
         ]
        }
       ],
       "layout": {
        "title": ""
       }
      },
      "text/html": [
       "<div id=\"24450967-9e16-4580-b878-26d2e2c03aa6\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"24450967-9e16-4580-b878-26d2e2c03aa6\", [{\"type\": \"scatter\", \"y\": [1.2904061747198659, 1.3721451570896488, 1.4830813460147056, 1.5460726902950677, 1.6284520655555037, 1.6995525521693113, 1.74781800172497, 1.7220958161918305, 1.8229461110464957, 1.8619881936531288, 1.8400301256290792, 1.9009503487666801, 1.863105614621909, 1.8905083710603818, 1.8650219855171717, 1.8633904019386527, 1.8303953462799623, 1.800923267496681, 1.7789319572775326, 1.730680828930675, 1.619730451287393, 1.6153482963340446, 1.571091391281538, 1.4824826680840257, 1.3922855447531204, 1.317888893794123, 1.229534125377882, 1.1040121477615608, 1.0423947208051123, 0.9514310527132379, 0.8073624309750395, 0.7072861235454271, 0.5530222120632857, 0.425808892837975, 0.31523943128977017, 0.20548825381884275, 0.10333033338382572, -0.031350115198373274, -0.1615014780668712, -0.24195778091388642, -0.4229236876642579, -0.551388046844491, -0.652503672441579, -0.8021848909205408, -0.8854316472837753, -0.9803064505745986, -1.0807471716177819, -1.2063958069233878, -1.2913815152156034, -1.373695468248517, -1.4730893754261052, -1.5416883221279662, -1.6185261342832578, -1.6816251833008593, -1.7483426685728587, -1.7846126367180093, -1.8585961949319496, -1.8740397294270845, -1.9166498085506656, -1.9068933698428499, -1.940586699842234, -1.9315952496754316, -1.9473290057766413, -1.9449617257317346, -1.9094189883280503, -1.8976558993607016, -1.8265699054095772, -1.7862466252194933, -1.7451158230903077, -1.6716399216848434, -1.6325876321786112, -1.5579727584389371, -1.5033298574540046, -1.4200995826437433, -1.360493029387998, -1.2127841246283435, -1.1435360321981802, -1.0584812686901863, -0.9494868573745819, -0.8385133734059343, -0.6963778702708578, -0.5943281811614144, -0.47905057582344274, -0.3590833160905554, -0.25402742669233247, -0.1584063674114831, 0.009825792596144503, 0.1186990680583482, 0.22267193051376535, 0.39099821009118335, 0.47367211813382515, 0.5813563623937186, 0.714780627194707, 0.8195561610996839, 0.8705652501431911, 1.0196729999322327, 1.0842013384893507, 1.2017480391512587, 1.2670581387201654, 1.3774367555440292], \"x\": [0.0, 0.010000199999999999, 0.020000399999999998, 0.030000599999999995, 0.040000799999999996, 0.050001, 0.06000119999999999, 0.07000139999999999, 0.08000159999999999, 0.09000179999999999, 0.100002, 0.1100022, 0.12000239999999998, 0.1300026, 0.14000279999999998, 0.150003, 0.16000319999999998, 0.17000339999999997, 0.18000359999999999, 0.19000379999999997, 0.200004, 0.21000419999999997, 0.2200044, 0.23000459999999998, 0.24000479999999996, 0.250005, 0.2600052, 0.27000539999999995, 0.28000559999999997, 0.2900058, 0.300006, 0.31000619999999995, 0.32000639999999997, 0.3300066, 0.34000679999999994, 0.35000699999999996, 0.36000719999999997, 0.3700074, 0.38000759999999995, 0.39000779999999996, 0.400008, 0.41000819999999993, 0.42000839999999995, 0.43000859999999996, 0.4400088, 0.45000899999999994, 0.46000919999999995, 0.47000939999999997, 0.4800095999999999, 0.49000979999999994, 0.50001, 0.5100102, 0.5200104, 0.5300106, 0.5400107999999999, 0.5500109999999999, 0.5600111999999999, 0.5700114, 0.5800116, 0.5900118, 0.600012, 0.6100121999999999, 0.6200123999999999, 0.6300125999999999, 0.6400127999999999, 0.650013, 0.6600132, 0.6700134, 0.6800135999999999, 0.6900137999999999, 0.7000139999999999, 0.7100141999999999, 0.7200143999999999, 0.7300146, 0.7400148, 0.7500149999999999, 0.7600151999999999, 0.7700153999999999, 0.7800155999999999, 0.7900157999999999, 0.800016, 0.8100162, 0.8200163999999999, 0.8300165999999999, 0.8400167999999999, 0.8500169999999999, 0.8600171999999999, 0.8700173999999999, 0.8800176, 0.8900177999999999, 0.9000179999999999, 0.9100181999999999, 0.9200183999999999, 0.9300185999999999, 0.9400187999999999, 0.950019, 0.9600191999999999, 0.9700193999999999, 0.9800195999999999, 0.9900197999999999]}], {\"title\": \"\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"24450967-9e16-4580-b878-26d2e2c03aa6\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"24450967-9e16-4580-b878-26d2e2c03aa6\", [{\"type\": \"scatter\", \"y\": [1.2904061747198659, 1.3721451570896488, 1.4830813460147056, 1.5460726902950677, 1.6284520655555037, 1.6995525521693113, 1.74781800172497, 1.7220958161918305, 1.8229461110464957, 1.8619881936531288, 1.8400301256290792, 1.9009503487666801, 1.863105614621909, 1.8905083710603818, 1.8650219855171717, 1.8633904019386527, 1.8303953462799623, 1.800923267496681, 1.7789319572775326, 1.730680828930675, 1.619730451287393, 1.6153482963340446, 1.571091391281538, 1.4824826680840257, 1.3922855447531204, 1.317888893794123, 1.229534125377882, 1.1040121477615608, 1.0423947208051123, 0.9514310527132379, 0.8073624309750395, 0.7072861235454271, 0.5530222120632857, 0.425808892837975, 0.31523943128977017, 0.20548825381884275, 0.10333033338382572, -0.031350115198373274, -0.1615014780668712, -0.24195778091388642, -0.4229236876642579, -0.551388046844491, -0.652503672441579, -0.8021848909205408, -0.8854316472837753, -0.9803064505745986, -1.0807471716177819, -1.2063958069233878, -1.2913815152156034, -1.373695468248517, -1.4730893754261052, -1.5416883221279662, -1.6185261342832578, -1.6816251833008593, -1.7483426685728587, -1.7846126367180093, -1.8585961949319496, -1.8740397294270845, -1.9166498085506656, -1.9068933698428499, -1.940586699842234, -1.9315952496754316, -1.9473290057766413, -1.9449617257317346, -1.9094189883280503, -1.8976558993607016, -1.8265699054095772, -1.7862466252194933, -1.7451158230903077, -1.6716399216848434, -1.6325876321786112, -1.5579727584389371, -1.5033298574540046, -1.4200995826437433, -1.360493029387998, -1.2127841246283435, -1.1435360321981802, -1.0584812686901863, -0.9494868573745819, -0.8385133734059343, -0.6963778702708578, -0.5943281811614144, -0.47905057582344274, -0.3590833160905554, -0.25402742669233247, -0.1584063674114831, 0.009825792596144503, 0.1186990680583482, 0.22267193051376535, 0.39099821009118335, 0.47367211813382515, 0.5813563623937186, 0.714780627194707, 0.8195561610996839, 0.8705652501431911, 1.0196729999322327, 1.0842013384893507, 1.2017480391512587, 1.2670581387201654, 1.3774367555440292], \"x\": [0.0, 0.010000199999999999, 0.020000399999999998, 0.030000599999999995, 0.040000799999999996, 0.050001, 0.06000119999999999, 0.07000139999999999, 0.08000159999999999, 0.09000179999999999, 0.100002, 0.1100022, 0.12000239999999998, 0.1300026, 0.14000279999999998, 0.150003, 0.16000319999999998, 0.17000339999999997, 0.18000359999999999, 0.19000379999999997, 0.200004, 0.21000419999999997, 0.2200044, 0.23000459999999998, 0.24000479999999996, 0.250005, 0.2600052, 0.27000539999999995, 0.28000559999999997, 0.2900058, 0.300006, 0.31000619999999995, 0.32000639999999997, 0.3300066, 0.34000679999999994, 0.35000699999999996, 0.36000719999999997, 0.3700074, 0.38000759999999995, 0.39000779999999996, 0.400008, 0.41000819999999993, 0.42000839999999995, 0.43000859999999996, 0.4400088, 0.45000899999999994, 0.46000919999999995, 0.47000939999999997, 0.4800095999999999, 0.49000979999999994, 0.50001, 0.5100102, 0.5200104, 0.5300106, 0.5400107999999999, 0.5500109999999999, 0.5600111999999999, 0.5700114, 0.5800116, 0.5900118, 0.600012, 0.6100121999999999, 0.6200123999999999, 0.6300125999999999, 0.6400127999999999, 0.650013, 0.6600132, 0.6700134, 0.6800135999999999, 0.6900137999999999, 0.7000139999999999, 0.7100141999999999, 0.7200143999999999, 0.7300146, 0.7400148, 0.7500149999999999, 0.7600151999999999, 0.7700153999999999, 0.7800155999999999, 0.7900157999999999, 0.800016, 0.8100162, 0.8200163999999999, 0.8300165999999999, 0.8400167999999999, 0.8500169999999999, 0.8600171999999999, 0.8700173999999999, 0.8800176, 0.8900177999999999, 0.9000179999999999, 0.9100181999999999, 0.9200183999999999, 0.9300185999999999, 0.9400187999999999, 0.950019, 0.9600191999999999, 0.9700193999999999, 0.9800195999999999, 0.9900197999999999]}], {\"title\": \"\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s_i = 0\n",
    "e_i = s_i + 100\n",
    "plotly.offline.iplot({\n",
    "    \"data\": [Scatter(x=signal_time[s_i:e_i],y=signal_amp[s_i:e_i])],\n",
    "    \"layout\": Layout(title=\"\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 50\n",
    "prediction_length = 1\n",
    "input_feature_count = 1\n",
    "output_feature_count = 1\n",
    "batch_size = 512\n",
    "hidden_count_per_layer = [50,50]\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [batch_size, sequence_length, input_feature_count], name = 'inputs')\n",
    "targets = tf.placeholder(tf.float32, [batch_size, output_feature_count], name = 'targets')\n",
    "keep_prob = tf.placeholder(tf.float32, name = 'keep')\n",
    "learning_rate = tf.placeholder(tf.float32, name = 'learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "\n",
    "\n",
    "for hidden_count in hidden_count_per_layer:\n",
    "    layer =  tf.nn.rnn_cell.LSTMCell(hidden_count, state_is_tuple=True)\n",
    "    layer_with_dropout = tf.nn.rnn_cell.DropoutWrapper(layer,\n",
    "                                          input_keep_prob=keep_prob,\n",
    "                                          output_keep_prob=1.0)\n",
    "    layers.append(layer)\n",
    "hidden_network = tf.nn.rnn_cell.MultiRNNCell(layers, state_is_tuple=True)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_state_size(network,batch_size):\n",
    "    states = 0\n",
    "    for layer_size in hidden_network.state_size:\n",
    "        states += layer_size[0]\n",
    "        states += layer_size[1]\n",
    "    return states * batch_size\n",
    "    \n",
    "def pack_state_tuple(states_per_layer):\n",
    "    return tf.concat(axis=0, values=[tf.reshape(layer_states, (-1,1)) for layer_states in states_per_layer])\n",
    "\n",
    "def unpack_state_tuple(network, batch_size, packed_states):\n",
    "    rnn_list_state = []\n",
    "    start_index = 0\n",
    "\n",
    "    for layer_size in hidden_network.state_size:\n",
    "            states_cnt = batch_size * layer_size[0]\n",
    "            #print(\"layer\",layer_size)\n",
    "            # add a tuple with 2 tensors (batch_size, hidden_count )\n",
    "            #print(\"c\", packed_states.shape, start_index, states_cnt)\n",
    "            states_c = tf.slice(packed_states, begin=[start_index,0], size=[states_cnt,1])\n",
    "            states_c = tf.reshape(states_c,[batch_size, layer_size[0]])\n",
    "            start_index += states_cnt\n",
    "            states_cnt = batch_size * layer_size[1]\n",
    "            #print(\"h\", packed_states.shape, start_index, states_cnt)\n",
    "            states_h = tf.slice(initial_state, begin=[start_index,0], size=[states_cnt,1])\n",
    "            states_h = tf.reshape(states_h,[batch_size, layer_size[1]])\n",
    "            start_index += states_cnt\n",
    "            \n",
    "            rnn_list_state.append(tf.nn.rnn_cell.LSTMStateTuple(states_c,states_h))\n",
    "    tuple_state=tuple(rnn_list_state)            \n",
    "    return tuple_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states in network 102400\n"
     ]
    }
   ],
   "source": [
    "sz = get_network_state_size(hidden_network,batch_size)\n",
    "print(\"states in network\", sz)\n",
    "\n",
    "zero_state = pack_state_tuple(hidden_network.zero_state(batch_size, tf.float32))\n",
    "initial_state = tf.placeholder_with_default(\n",
    "    zero_state, \n",
    "    (sz,1), \n",
    "    name=\"initial_state\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff 0.0\n"
     ]
    }
   ],
   "source": [
    "# Test to check that the pack and unpack functions are eachothers inverse\n",
    "\n",
    "#copy of the input (packed) shape (x,1)\n",
    "initial_copy = tf.slice(initial_state,(0,0),(sz,1))\n",
    "\n",
    "#unpack into (LSTMTuple(),LSTMTuple(),....)\n",
    "unpacked = unpack_state_tuple(hidden_network,batch_size,initial_copy)\n",
    "rnn_tuple_state = unpacked\n",
    "#repack into vector (x,1)\n",
    "packed_again = pack_state_tuple(unpacked)\n",
    "\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    #v_outputs, v_state = sess.run([outputs,state], feed_dict={inputs: batch_inputs, targets: batch_targets})\n",
    "    cp = sess.run(initial_copy,  feed_dict={}) #initial_state: initial_state_input})\n",
    "    #cp[0] = 1\n",
    "    cp[:,0] = np.linspace(start=0,stop=cp.shape[0],num=cp.shape[0])\n",
    "    \n",
    "    cp,up,cp2 = sess.run([initial_copy,unpacked,packed_again],  feed_dict={initial_state: cp}) #initial_state: initial_state_input})\n",
    "    #print(\"cp\",cp.shape,cp)\n",
    "    #print(\"up\",unpacked,up)\n",
    "    #print(\"cp2\",packed_again,cp2)\n",
    "    #print(\"up2\",unpacked_again,up2)\n",
    "    diff = cp - cp2\n",
    "    print(\"diff\",np.sum(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs  (512, 50, 1)\n",
      "outputs before transpose (512, 50, 50)\n",
      "outputs after transpose (50, 512, 50)\n",
      "last output (512, 50)\n",
      "prediction (512, 1)\n",
      "targets (512, 1)\n"
     ]
    }
   ],
   "source": [
    "#out_weights=tf.Variable(tf.random_normal([hidden_count_per_layer[-1],output_feature_count]))\n",
    "#out_bias=tf.Variable(tf.random_normal([output_feature_count]))\n",
    "print(\"inputs \",inputs.shape)\n",
    "outputs, state = tf.nn.dynamic_rnn(hidden_network, inputs, dtype=tf.float32, initial_state=rnn_tuple_state, )\n",
    "state_packed = pack_state_tuple(state)\n",
    "print(\"outputs before transpose\", outputs.shape)\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "print(\"outputs after transpose\", outputs.shape)\n",
    "last_output = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)\n",
    "print(\"last output\", last_output.shape)\n",
    "                                   \n",
    "#out_size = target.get_shape()[2].value\n",
    "predictions = tf.contrib.layers.fully_connected(last_output, output_feature_count, activation_fn=None)\n",
    "print(\"prediction\", predictions.shape)\n",
    "print(\"targets\", targets.shape)\n",
    "#prediction = tf.nn.softmax(logit)\n",
    "#loss = tf.losses.softmax_cross_entropy(target, logit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(tf.squared_difference(predictions, targets))\n",
    "\n",
    "#optimization\n",
    "opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size 49949\n",
      "29971 Examples (58 batches) in train set\n",
      "9989 Examples (19 batches) in dev set\n",
      "9989 Examples (19 batches) in test set\n"
     ]
    }
   ],
   "source": [
    "start_indices = np.linspace(\n",
    "    0,\n",
    "    sample_length-sequence_length-prediction_length-1,\n",
    "    sample_length-sequence_length-prediction_length-1, dtype= np.int32)\n",
    "\n",
    "dev_size_perc = 0.20\n",
    "test_size_perc = 0.20\n",
    "\n",
    "dev_size = int(np.floor(start_indices.shape[0] * dev_size_perc))\n",
    "test_size  = int(np.floor(start_indices.shape[0] * test_size_perc))\n",
    "train_size = start_indices.shape[0] - test_size - dev_size\n",
    "train_batch_count = int(np.floor(train_size / batch_size))\n",
    "dev_batch_count = int(np.floor(dev_size / batch_size))\n",
    "test_batch_count = int(np.floor(test_size / batch_size))\n",
    "\n",
    "print(\"dataset size %d\" %(start_indices.shape[0]))\n",
    "print(\"%d Examples (%d batches) in train set\" %(train_size, train_batch_count))\n",
    "print(\"%d Examples (%d batches) in dev set\" %(dev_size,dev_batch_count))\n",
    "print(\"%d Examples (%d batches) in test set\" %(test_size,test_batch_count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 50, 1) (512, 1)\n",
      "(50, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle (start_indices)\n",
    "train_indices = start_indices[0:int(train_size)]\n",
    "dev_indices= start_indices[int(train_size):int(train_size+dev_size)]\n",
    "test_indices = start_indices[int(train_size+dev_size):int(train_size+dev_size+test_size)]\n",
    "\n",
    "def get_batch(batch_index, indexes, size=batch_size):\n",
    "    batch_start_indexes = indexes[batch_index*size:batch_index*size+size]\n",
    "    batch_inputs = np.zeros((size,sequence_length, input_feature_count))\n",
    "    batch_targets = np.zeros((size,prediction_length))\n",
    "    for i in range(size):\n",
    "        se = batch_start_indexes[i]\n",
    "        part = signal_amp[se:se+sequence_length]\n",
    "        batch_inputs[i,0:sequence_length,0] = part\n",
    "        batch_targets[i,0] = signal_amp[se+sequence_length+1]\n",
    "\n",
    "    return batch_inputs,batch_targets\n",
    "\n",
    "batch_inputs,batch_targets = get_batch(train_batch_count-1,train_indices)\n",
    "print(batch_inputs.shape,batch_targets.shape)\n",
    "\n",
    "example_inputs = batch_inputs[0,:,:]\n",
    "example_targets =  batch_targets[0,:]\n",
    "print(example_inputs.shape)\n",
    "#b_i = 1\n",
    "#b_s = batch_inputs[b_i,0:sequence_length,0]\n",
    "#plotly.offline.iplot({\n",
    "#    \"data\": [Scatter(y=b_s)],\n",
    "#    \"layout\": Layout(title=\"\")\n",
    "#})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch input shape (512, 50, 1)\n",
      "(512, 1)\n",
      "[-0.11653353] [ 0.58121891]\n",
      "288.75 [-0.11647449] [ 0.58121891]\n",
      "225.513 [ 0.07935123] [ 0.58121891]\n",
      "177.012 [ 0.20965971] [ 0.58121891]\n",
      "129.561 [ 0.38865343] [ 0.58121891]\n",
      "96.3236 [ 0.60352397] [ 0.58121891]\n",
      "66.3575 [ 0.72608513] [ 0.58121891]\n",
      "35.9544 [ 0.69287181] [ 0.58121891]\n",
      "16.9334 [ 0.62053478] [ 0.58121891]\n",
      "11.0414 [ 0.612423] [ 0.58121891]\n",
      "8.85931 [ 0.67310977] [ 0.58121891]\n",
      "16.8448 [ 0.82962584] [ 0.58121891]\n",
      "13.4691 [ 0.70581067] [ 0.58121891]\n",
      "11.2271 [ 0.78637755] [ 0.58121891]\n",
      "7.71416 [ 0.58813578] [ 0.58121891]\n",
      "4.58144 [ 0.66887516] [ 0.58121891]\n",
      "6.86396 [ 0.78906053] [ 0.58121891]\n",
      "6.8814 [ 0.73353046] [ 0.58121891]\n",
      "6.7304 [ 0.67206466] [ 0.58121891]\n",
      "5.39651 [ 0.68908876] [ 0.58121891]\n",
      "5.03756 [ 0.66945201] [ 0.58121891]\n",
      "3.43868 [ 0.60664016] [ 0.58121891]\n",
      "2.35383 [ 0.60541523] [ 0.58121891]\n",
      "1.7797 [ 0.66800594] [ 0.58121891]\n",
      "1.87366 [ 0.67074525] [ 0.58121891]\n",
      "1.48717 [ 0.59801883] [ 0.58121891]\n",
      "1.27699 [ 0.56348521] [ 0.58121891]\n",
      "1.64299 [ 0.57864022] [ 0.58121891]\n",
      "1.45849 [ 0.56039447] [ 0.58121891]\n",
      "1.23835 [ 0.53823322] [ 0.58121891]\n",
      "0.77403 [ 0.58754349] [ 0.58121891]\n",
      "1.03194 [ 0.63690537] [ 0.58121891]\n",
      "0.641701 [ 0.60658747] [ 0.58121891]\n",
      "0.624359 [ 0.5830695] [ 0.58121891]\n",
      "0.648136 [ 0.61049742] [ 0.58121891]\n",
      "0.720952 [ 0.60974228] [ 0.58121891]\n",
      "0.648905 [ 0.58790737] [ 0.58121891]\n",
      "0.442939 [ 0.61740828] [ 0.58121891]\n",
      "0.559499 [ 0.65179044] [ 0.58121891]\n",
      "0.340905 [ 0.62578946] [ 0.58121891]\n",
      "0.258947 [ 0.6003722] [ 0.58121891]\n",
      "0.215755 [ 0.60974759] [ 0.58121891]\n",
      "0.248647 [ 0.60040385] [ 0.58121891]\n",
      "0.266375 [ 0.58065611] [ 0.58121891]\n",
      "0.223586 [ 0.59965944] [ 0.58121891]\n",
      "0.351397 [ 0.62038726] [ 0.58121891]\n",
      "0.289062 [ 0.60131168] [ 0.58121891]\n",
      "0.258845 [ 0.58865744] [ 0.58121891]\n",
      "0.245748 [ 0.59980071] [ 0.58121891]\n",
      "0.191093 [ 0.59449959] [ 0.58121891]\n",
      "0.162726 [ 0.58903688] [ 0.58121891]\n",
      "0.112642 [ 0.61255395] [ 0.58121891]\n",
      "0.140342 [ 0.62205315] [ 0.58121891]\n",
      "0.126142 [ 0.60485274] [ 0.58121891]\n",
      "0.0977033 [ 0.60487413] [ 0.58121891]\n",
      "0.136756 [ 0.60889453] [ 0.58121891]\n",
      "0.111355 [ 0.59621054] [ 0.58121891]\n",
      "0.0865637 [ 0.60056627] [ 0.58121891]\n",
      "0.0981575 [ 0.6141845] [ 0.58121891]\n",
      "0.0729349 [ 0.60236931] [ 0.58121891]\n",
      "0.0603258 [ 0.5930869] [ 0.58121891]\n",
      "0.0687744 [ 0.59602755] [ 0.58121891]\n",
      "0.0610347 [ 0.5854001] [ 0.58121891]\n",
      "0.0606952 [ 0.58340526] [ 0.58121891]\n",
      "0.0648976 [ 0.59607542] [ 0.58121891]\n",
      "0.0578548 [ 0.5911603] [ 0.58121891]\n",
      "0.0523146 [ 0.58587635] [ 0.58121891]\n",
      "0.0494777 [ 0.59085262] [ 0.58121891]\n",
      "0.0414422 [ 0.58424675] [ 0.58121891]\n",
      "0.0368503 [ 0.58554173] [ 0.58121891]\n",
      "0.0390853 [ 0.59718561] [ 0.58121891]\n",
      "0.0374853 [ 0.59364396] [ 0.58121891]\n",
      "0.034511 [ 0.59320515] [ 0.58121891]\n",
      "0.0378502 [ 0.5959273] [ 0.58121891]\n",
      "0.0366293 [ 0.58822137] [ 0.58121891]\n",
      "0.0287446 [ 0.59217715] [ 0.58121891]\n",
      "0.0277166 [ 0.5968256] [ 0.58121891]\n",
      "0.0277948 [ 0.59160537] [ 0.58121891]\n",
      "0.0214461 [ 0.59440517] [ 0.58121891]\n",
      "0.0198343 [ 0.59012336] [ 0.58121891]\n",
      "0.0227295 [ 0.58599299] [ 0.58121891]\n",
      "0.0222798 [ 0.59203768] [ 0.58121891]\n",
      "0.0213633 [ 0.58939642] [ 0.58121891]\n",
      "0.0203406 [ 0.59147048] [ 0.58121891]\n",
      "0.0193211 [ 0.59178579] [ 0.58121891]\n",
      "0.0201627 [ 0.58675873] [ 0.58121891]\n",
      "0.0188654 [ 0.59169763] [ 0.58121891]\n",
      "0.0165428 [ 0.59037626] [ 0.58121891]\n",
      "0.0165488 [ 0.59277922] [ 0.58121891]\n",
      "0.0165472 [ 0.59381551] [ 0.58121891]\n",
      "0.0168797 [ 0.5892325] [ 0.58121891]\n",
      "0.0170573 [ 0.59216189] [ 0.58121891]\n",
      "0.0154823 [ 0.58893871] [ 0.58121891]\n",
      "0.014459 [ 0.59196115] [ 0.58121891]\n",
      "0.013892 [ 0.59054321] [ 0.58121891]\n",
      "0.0134698 [ 0.58832073] [ 0.58121891]\n",
      "0.0141258 [ 0.58873117] [ 0.58121891]\n",
      "0.0144057 [ 0.58558345] [ 0.58121891]\n",
      "0.0144995 [ 0.58956242] [ 0.58121891]\n",
      "0.0146321 [ 0.58614683] [ 0.58121891]\n",
      "0.0141121 [ 0.58922446] [ 0.58121891]\n",
      "0.0138441 [ 0.58483136] [ 0.58121891]\n",
      "0.0136015 [ 0.58843446] [ 0.58121891]\n",
      "0.0133615 [ 0.58569407] [ 0.58121891]\n",
      "0.0136589 [ 0.59007555] [ 0.58121891]\n",
      "0.0139889 [ 0.58568954] [ 0.58121891]\n",
      "0.0148888 [ 0.59000039] [ 0.58121891]\n",
      "0.0167276 [ 0.58345056] [ 0.58121891]\n",
      "0.0204483 [ 0.5922026] [ 0.58121891]\n",
      "0.0289998 [ 0.58070493] [ 0.58121891]\n",
      "0.0488861 [ 0.59735537] [ 0.58121891]\n",
      "0.0955732 [ 0.57122678] [ 0.58121891]\n",
      "0.211389 [ 0.60957181] [ 0.58121891]\n",
      "0.476319 [ 0.5505482] [ 0.58121891]\n",
      "1.08881 [ 0.64069486] [ 0.58121891]\n",
      "1.97742 [ 0.51335174] [ 0.58121891]\n",
      "2.8067 [ 0.67095852] [ 0.58121891]\n",
      "1.55147 [ 0.52229285] [ 0.58121891]\n",
      "0.104584 [ 0.60220122] [ 0.58121891]\n",
      "0.524469 [ 0.6266284] [ 0.58121891]\n",
      "1.23318 [ 0.53684533] [ 0.58121891]\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle (start_indices)\n",
    "train_indices = start_indices[0:int(train_size)]\n",
    "dev_indices= start_indices[int(train_size):int(train_size+dev_size)]\n",
    "test_indices = start_indices[int(train_size+dev_size):int(train_size+dev_size+test_size)]\n",
    "\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    np.random.shuffle (train_indices)\n",
    "    \n",
    "    batch_inputs,batch_targets = get_batch(0, train_indices)\n",
    "    print(\"batch input shape\", batch_inputs.shape)\n",
    "    #v_outputs, v_state = sess.run([outputs,state], feed_dict={inputs: batch_inputs, targets: batch_targets})\n",
    "    v_predictions, v_state, v_state_packed = sess.run([predictions,state, state_packed], \n",
    "                                      feed_dict={\n",
    "                                          inputs: batch_inputs, \n",
    "                                          targets: batch_targets\n",
    "                                      })\n",
    "    print(v_predictions.shape)\n",
    "    print(v_predictions[0],batch_targets[0])\n",
    "    for i in range(0,120):\n",
    "        v_predictions, v_outputs, v_state, v_state_packed, v_loss, v_opt = sess.run(\n",
    "            [predictions, outputs, state, state_packed, loss, opt], \n",
    "            feed_dict={\n",
    "                learning_rate: 0.02, \n",
    "                inputs: batch_inputs, \n",
    "                targets: batch_targets,\n",
    "                initial_state: v_state_packed\n",
    "            }) #})\n",
    "        print(v_loss,v_predictions[0],batch_targets[0])\n",
    " \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Dev results batch 0, loss 458.682\n",
      "  Dev results batch 1, loss 525.952\n",
      "  Dev results batch 2, loss 519.397\n",
      "  Dev results batch 3, loss 475.219\n",
      "  Dev results batch 4, loss 558.162\n",
      "  Dev results batch 5, loss 449.233\n",
      "  Dev results batch 6, loss 510.851\n",
      "  Dev results batch 7, loss 514.147\n",
      "  Dev results batch 8, loss 508.348\n",
      "  Dev results batch 9, loss 503.836\n",
      "  Dev results batch 10, loss 496.116\n",
      "  Dev results batch 11, loss 508.366\n",
      "  Dev results batch 12, loss 492.309\n",
      "  Dev results batch 13, loss 512.456\n",
      "  Dev results batch 14, loss 503.431\n",
      "  Dev results batch 15, loss 514.911\n",
      "  Dev results batch 16, loss 510.841\n",
      "  Dev results batch 17, loss 544.195\n",
      "  Dev results batch 18, loss 485.198\n",
      "Dev results epoch start, loss 0.960221178599\n",
      "  Train results batch 0, loss 493.044\n",
      "  Train results batch 1, loss 537.421\n",
      "  Train results batch 2, loss 492.49\n",
      "  Train results batch 3, loss 317.076\n",
      "  Train results batch 4, loss 218.079\n",
      "  Train results batch 5, loss 150.922\n",
      "  Train results batch 6, loss 43.3711\n",
      "  Train results batch 7, loss 51.4745\n",
      "  Train results batch 8, loss 53.0\n",
      "  Train results batch 9, loss 58.1469\n",
      "  Train results batch 10, loss 15.6281\n",
      "  Train results batch 11, loss 15.5209\n",
      "  Train results batch 12, loss 21.9246\n",
      "  Train results batch 13, loss 13.7708\n",
      "  Train results batch 14, loss 14.7624\n",
      "  Train results batch 15, loss 20.9081\n",
      "  Train results batch 16, loss 12.5237\n",
      "  Train results batch 17, loss 5.93977\n",
      "  Train results batch 18, loss 6.79349\n",
      "  Train results batch 19, loss 13.0375\n",
      "  Train results batch 20, loss 5.45588\n",
      "  Train results batch 21, loss 4.75144\n",
      "  Train results batch 22, loss 8.33971\n",
      "  Train results batch 23, loss 7.11343\n",
      "  Train results batch 24, loss 1.21644\n",
      "  Train results batch 25, loss 3.83179\n",
      "  Train results batch 26, loss 5.69089\n",
      "  Train results batch 27, loss 1.82839\n",
      "  Train results batch 28, loss 1.36475\n",
      "  Train results batch 29, loss 4.19959\n",
      "  Train results batch 30, loss 3.14408\n",
      "  Train results batch 31, loss 1.57441\n",
      "  Train results batch 32, loss 1.59678\n",
      "  Train results batch 33, loss 2.88089\n",
      "  Train results batch 34, loss 2.17487\n",
      "  Train results batch 35, loss 0.834748\n",
      "  Train results batch 36, loss 1.0633\n",
      "  Train results batch 37, loss 1.97904\n",
      "  Train results batch 38, loss 1.31346\n",
      "  Train results batch 39, loss 0.667769\n",
      "  Train results batch 40, loss 1.09233\n",
      "  Train results batch 41, loss 1.39423\n",
      "  Train results batch 42, loss 1.37547\n",
      "  Train results batch 43, loss 0.730607\n",
      "  Train results batch 44, loss 0.884853\n",
      "  Train results batch 45, loss 1.26169\n",
      "  Train results batch 46, loss 0.917574\n",
      "  Train results batch 47, loss 0.689399\n",
      "  Train results batch 48, loss 0.948261\n",
      "  Train results batch 49, loss 0.868432\n",
      "  Train results batch 50, loss 0.816766\n",
      "  Train results batch 51, loss 0.514934\n",
      "  Train results batch 52, loss 0.670695\n",
      "  Train results batch 53, loss 0.813897\n",
      "  Train results batch 54, loss 0.688445\n",
      "  Train results batch 55, loss 0.638936\n",
      "  Train results batch 56, loss 0.648013\n",
      "  Train results batch 57, loss 0.724501\n",
      "Training results epoch 0, loss 0.087836024025\n",
      "  Dev results batch 0, loss 0.601772\n",
      "  Dev results batch 1, loss 0.624756\n",
      "  Dev results batch 2, loss 0.606508\n",
      "  Dev results batch 3, loss 0.604646\n",
      "  Dev results batch 4, loss 0.621857\n",
      "  Dev results batch 5, loss 0.52245\n",
      "  Dev results batch 6, loss 0.579352\n",
      "  Dev results batch 7, loss 0.573931\n",
      "  Dev results batch 8, loss 0.623409\n",
      "  Dev results batch 9, loss 0.640434\n",
      "  Dev results batch 10, loss 0.605731\n",
      "  Dev results batch 11, loss 0.582281\n",
      "  Dev results batch 12, loss 0.537669\n",
      "  Dev results batch 13, loss 0.666108\n",
      "  Dev results batch 14, loss 0.708815\n",
      "  Dev results batch 15, loss 0.716096\n",
      "  Dev results batch 16, loss 0.678319\n",
      "  Dev results batch 17, loss 0.686224\n",
      "  Dev results batch 18, loss 0.526257\n",
      "Dev results epoch 0, loss 0.00117195072636\n",
      "  Train results batch 0, loss 0.596834\n",
      "  Train results batch 1, loss 0.532497\n",
      "  Train results batch 2, loss 0.697456\n",
      "  Train results batch 3, loss 0.658855\n",
      "  Train results batch 4, loss 0.461513\n",
      "  Train results batch 5, loss 0.500353\n",
      "  Train results batch 6, loss 0.662149\n",
      "  Train results batch 7, loss 0.498201\n",
      "  Train results batch 8, loss 0.51397\n",
      "  Train results batch 9, loss 0.531503\n",
      "  Train results batch 10, loss 0.597176\n",
      "  Train results batch 11, loss 0.509201\n",
      "  Train results batch 12, loss 0.45341\n",
      "  Train results batch 13, loss 0.600827\n",
      "  Train results batch 14, loss 0.497841\n",
      "  Train results batch 15, loss 0.434282\n",
      "  Train results batch 16, loss 0.559645\n",
      "  Train results batch 17, loss 0.507697\n",
      "  Train results batch 18, loss 0.451404\n",
      "  Train results batch 19, loss 0.490002\n",
      "  Train results batch 20, loss 0.53986\n",
      "  Train results batch 21, loss 0.513459\n",
      "  Train results batch 22, loss 0.442044\n",
      "  Train results batch 23, loss 0.528994\n",
      "  Train results batch 24, loss 0.47241\n",
      "  Train results batch 25, loss 0.498299\n",
      "  Train results batch 26, loss 0.453827\n",
      "  Train results batch 27, loss 0.490503\n",
      "  Train results batch 28, loss 0.485283\n",
      "  Train results batch 29, loss 0.480084\n",
      "  Train results batch 30, loss 0.463172\n",
      "  Train results batch 31, loss 0.434786\n",
      "  Train results batch 32, loss 0.507736\n",
      "  Train results batch 33, loss 0.426564\n",
      "  Train results batch 34, loss 0.46374\n",
      "  Train results batch 35, loss 0.454738\n",
      "  Train results batch 36, loss 0.462659\n",
      "  Train results batch 37, loss 0.490908\n",
      "  Train results batch 38, loss 0.458034\n",
      "  Train results batch 39, loss 0.475917\n",
      "  Train results batch 40, loss 0.433823\n",
      "  Train results batch 41, loss 0.472517\n",
      "  Train results batch 42, loss 0.444829\n",
      "  Train results batch 43, loss 0.472693\n",
      "  Train results batch 44, loss 0.424484\n",
      "  Train results batch 45, loss 0.416372\n",
      "  Train results batch 46, loss 0.417867\n",
      "  Train results batch 47, loss 0.493876\n",
      "  Train results batch 48, loss 0.456625\n",
      "  Train results batch 49, loss 0.446174\n",
      "  Train results batch 50, loss 0.4391\n",
      "  Train results batch 51, loss 0.432451\n",
      "  Train results batch 52, loss 0.415097\n",
      "  Train results batch 53, loss 0.443138\n",
      "  Train results batch 54, loss 0.445963\n",
      "  Train results batch 55, loss 0.479596\n",
      "  Train results batch 56, loss 0.40975\n",
      "  Train results batch 57, loss 0.404754\n",
      "Training results epoch 1, loss 0.000942475790734\n",
      "  Dev results batch 0, loss 0.447443\n",
      "  Dev results batch 1, loss 0.471582\n",
      "  Dev results batch 2, loss 0.409043\n",
      "  Dev results batch 3, loss 0.414021\n",
      "  Dev results batch 4, loss 0.4497\n",
      "  Dev results batch 5, loss 0.392361\n",
      "  Dev results batch 6, loss 0.395525\n",
      "  Dev results batch 7, loss 0.422637\n",
      "  Dev results batch 8, loss 0.445913\n",
      "  Dev results batch 9, loss 0.433639\n",
      "  Dev results batch 10, loss 0.443194\n",
      "  Dev results batch 11, loss 0.445942\n",
      "  Dev results batch 12, loss 0.364205\n",
      "  Dev results batch 13, loss 0.449761\n",
      "  Dev results batch 14, loss 0.513173\n",
      "  Dev results batch 15, loss 0.474235\n",
      "  Dev results batch 16, loss 0.47717\n",
      "  Dev results batch 17, loss 0.461684\n",
      "  Dev results batch 18, loss 0.405159\n",
      "Dev results epoch 1, loss 0.000832554572384\n",
      "  Train results batch 0, loss 0.485066\n",
      "  Train results batch 1, loss 0.419002\n",
      "  Train results batch 2, loss 0.441328\n",
      "  Train results batch 3, loss 0.457626\n",
      "  Train results batch 4, loss 0.41173\n",
      "  Train results batch 5, loss 0.443855\n",
      "  Train results batch 6, loss 0.41624\n",
      "  Train results batch 7, loss 0.442233\n",
      "  Train results batch 8, loss 0.439747\n",
      "  Train results batch 9, loss 0.452692\n",
      "  Train results batch 10, loss 0.410811\n",
      "  Train results batch 11, loss 0.395956\n",
      "  Train results batch 12, loss 0.430177\n",
      "  Train results batch 13, loss 0.402449\n",
      "  Train results batch 14, loss 0.395595\n",
      "  Train results batch 15, loss 0.409965\n",
      "  Train results batch 16, loss 0.441394\n",
      "  Train results batch 17, loss 0.443235\n",
      "  Train results batch 18, loss 0.516897\n",
      "  Train results batch 19, loss 0.376007\n",
      "  Train results batch 20, loss 0.429974\n",
      "  Train results batch 21, loss 0.459291\n",
      "  Train results batch 22, loss 0.430285\n",
      "  Train results batch 23, loss 0.430761\n",
      "  Train results batch 24, loss 0.460191\n",
      "  Train results batch 25, loss 0.476848\n",
      "  Train results batch 26, loss 0.447699\n",
      "  Train results batch 27, loss 0.439263\n",
      "  Train results batch 28, loss 0.398266\n",
      "  Train results batch 29, loss 0.422092\n",
      "  Train results batch 30, loss 0.3751\n",
      "  Train results batch 31, loss 0.417152\n",
      "  Train results batch 32, loss 0.366726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train results batch 33, loss 0.439899\n",
      "  Train results batch 34, loss 0.42923\n",
      "  Train results batch 35, loss 0.479\n",
      "  Train results batch 36, loss 0.37423\n",
      "  Train results batch 37, loss 0.386146\n",
      "  Train results batch 38, loss 0.450983\n",
      "  Train results batch 39, loss 0.387242\n",
      "  Train results batch 40, loss 0.41585\n",
      "  Train results batch 41, loss 0.435805\n",
      "  Train results batch 42, loss 0.443473\n",
      "  Train results batch 43, loss 0.425311\n",
      "  Train results batch 44, loss 0.406952\n",
      "  Train results batch 45, loss 0.399488\n",
      "  Train results batch 46, loss 0.382407\n",
      "  Train results batch 47, loss 0.393976\n",
      "  Train results batch 48, loss 0.444474\n",
      "  Train results batch 49, loss 0.43777\n",
      "  Train results batch 50, loss 0.389165\n",
      "  Train results batch 51, loss 0.416331\n",
      "  Train results batch 52, loss 0.438589\n",
      "  Train results batch 53, loss 0.439667\n",
      "  Train results batch 54, loss 0.428943\n",
      "  Train results batch 55, loss 0.443978\n",
      "  Train results batch 56, loss 0.439431\n",
      "  Train results batch 57, loss 0.398493\n",
      "Training results epoch 2, loss 0.000824546480651\n",
      "  Dev results batch 0, loss 0.472719\n",
      "  Dev results batch 1, loss 0.465851\n",
      "  Dev results batch 2, loss 0.434821\n",
      "  Dev results batch 3, loss 0.434199\n",
      "  Dev results batch 4, loss 0.481838\n",
      "  Dev results batch 5, loss 0.414556\n",
      "  Dev results batch 6, loss 0.40476\n",
      "  Dev results batch 7, loss 0.460875\n",
      "  Dev results batch 8, loss 0.456733\n",
      "  Dev results batch 9, loss 0.437476\n",
      "  Dev results batch 10, loss 0.457775\n",
      "  Dev results batch 11, loss 0.45819\n",
      "  Dev results batch 12, loss 0.372854\n",
      "  Dev results batch 13, loss 0.464697\n",
      "  Dev results batch 14, loss 0.517808\n",
      "  Dev results batch 15, loss 0.48544\n",
      "  Dev results batch 16, loss 0.488807\n",
      "  Dev results batch 17, loss 0.451092\n",
      "  Dev results batch 18, loss 0.438951\n",
      "Dev results epoch 2, loss 0.000860891162914\n",
      "  Train results batch 0, loss 0.437927\n",
      "  Train results batch 1, loss 0.448216\n",
      "  Train results batch 2, loss 0.40803\n",
      "  Train results batch 3, loss 0.374752\n",
      "  Train results batch 4, loss 0.389674\n",
      "  Train results batch 5, loss 0.447688\n",
      "  Train results batch 6, loss 0.428718\n",
      "  Train results batch 7, loss 0.415248\n",
      "  Train results batch 8, loss 0.438097\n",
      "  Train results batch 9, loss 0.457296\n",
      "  Train results batch 10, loss 0.427285\n",
      "  Train results batch 11, loss 0.447205\n",
      "  Train results batch 12, loss 0.455882\n",
      "  Train results batch 13, loss 0.42362\n",
      "  Train results batch 14, loss 0.464832\n",
      "  Train results batch 15, loss 0.382264\n",
      "  Train results batch 16, loss 0.41866\n",
      "  Train results batch 17, loss 0.441037\n",
      "  Train results batch 18, loss 0.42299\n",
      "  Train results batch 19, loss 0.405765\n",
      "  Train results batch 20, loss 0.445274\n",
      "  Train results batch 21, loss 0.465191\n",
      "  Train results batch 22, loss 0.448605\n",
      "  Train results batch 23, loss 0.409492\n",
      "  Train results batch 24, loss 0.415521\n",
      "  Train results batch 25, loss 0.392329\n",
      "  Train results batch 26, loss 0.474099\n",
      "  Train results batch 27, loss 0.428384\n",
      "  Train results batch 28, loss 0.430488\n",
      "  Train results batch 29, loss 0.437384\n",
      "  Train results batch 30, loss 0.440463\n",
      "  Train results batch 31, loss 0.425953\n",
      "  Train results batch 32, loss 0.385296\n",
      "  Train results batch 33, loss 0.444262\n",
      "  Train results batch 34, loss 0.41715\n",
      "  Train results batch 35, loss 0.47924\n",
      "  Train results batch 36, loss 0.48801\n",
      "  Train results batch 37, loss 0.41047\n",
      "  Train results batch 38, loss 0.620683\n",
      "  Train results batch 39, loss 0.439211\n",
      "  Train results batch 40, loss 0.532116\n",
      "  Train results batch 41, loss 0.520062\n",
      "  Train results batch 42, loss 0.38834\n",
      "  Train results batch 43, loss 0.486271\n",
      "  Train results batch 44, loss 0.371854\n",
      "  Train results batch 45, loss 0.499433\n",
      "  Train results batch 46, loss 0.531669\n",
      "  Train results batch 47, loss 0.426597\n",
      "  Train results batch 48, loss 0.479136\n",
      "  Train results batch 49, loss 0.42099\n",
      "  Train results batch 50, loss 0.434344\n",
      "  Train results batch 51, loss 0.393022\n",
      "  Train results batch 52, loss 0.439122\n",
      "  Train results batch 53, loss 0.381982\n",
      "  Train results batch 54, loss 0.467875\n",
      "  Train results batch 55, loss 0.581374\n",
      "  Train results batch 56, loss 0.422942\n",
      "  Train results batch 57, loss 0.424654\n",
      "Training results epoch 3, loss 0.00085530934946\n",
      "  Dev results batch 0, loss 0.481794\n",
      "  Dev results batch 1, loss 0.458289\n",
      "  Dev results batch 2, loss 0.475544\n",
      "  Dev results batch 3, loss 0.455949\n",
      "  Dev results batch 4, loss 0.517637\n",
      "  Dev results batch 5, loss 0.427792\n",
      "  Dev results batch 6, loss 0.464378\n",
      "  Dev results batch 7, loss 0.466884\n",
      "  Dev results batch 8, loss 0.496116\n",
      "  Dev results batch 9, loss 0.500185\n",
      "  Dev results batch 10, loss 0.486632\n",
      "  Dev results batch 11, loss 0.439188\n",
      "  Dev results batch 12, loss 0.412882\n",
      "  Dev results batch 13, loss 0.485454\n",
      "  Dev results batch 14, loss 0.535073\n",
      "  Dev results batch 15, loss 0.539398\n",
      "  Dev results batch 16, loss 0.532411\n",
      "  Dev results batch 17, loss 0.498655\n",
      "  Dev results batch 18, loss 0.413794\n",
      "Dev results epoch 3, loss 0.000909806279428\n",
      "  Train results batch 0, loss 0.463424\n",
      "  Train results batch 1, loss 0.434451\n",
      "  Train results batch 2, loss 0.547786\n",
      "  Train results batch 3, loss 0.430073\n",
      "  Train results batch 4, loss 0.533172\n",
      "  Train results batch 5, loss 0.50378\n",
      "  Train results batch 6, loss 0.425305\n",
      "  Train results batch 7, loss 0.547735\n",
      "  Train results batch 8, loss 0.482563\n",
      "  Train results batch 9, loss 0.437816\n",
      "  Train results batch 10, loss 0.570561\n",
      "  Train results batch 11, loss 0.449486\n",
      "  Train results batch 12, loss 0.508384\n",
      "  Train results batch 13, loss 0.463602\n",
      "  Train results batch 14, loss 0.417822\n",
      "  Train results batch 15, loss 0.480169\n",
      "  Train results batch 16, loss 0.427887\n",
      "  Train results batch 17, loss 0.530572\n",
      "  Train results batch 18, loss 0.489753\n",
      "  Train results batch 19, loss 0.489642\n",
      "  Train results batch 20, loss 0.488654\n",
      "  Train results batch 21, loss 0.476416\n",
      "  Train results batch 22, loss 0.483615\n",
      "  Train results batch 23, loss 0.568314\n",
      "  Train results batch 24, loss 0.500992\n",
      "  Train results batch 25, loss 0.431419\n",
      "  Train results batch 26, loss 0.490497\n",
      "  Train results batch 27, loss 0.410717\n",
      "  Train results batch 28, loss 0.451187\n",
      "  Train results batch 29, loss 0.42573\n",
      "  Train results batch 30, loss 0.439482\n",
      "  Train results batch 31, loss 0.493557\n",
      "  Train results batch 32, loss 0.41816\n",
      "  Train results batch 33, loss 0.476029\n",
      "  Train results batch 34, loss 0.445933\n",
      "  Train results batch 35, loss 0.408053\n",
      "  Train results batch 36, loss 0.436851\n",
      "  Train results batch 37, loss 0.459635\n",
      "  Train results batch 38, loss 0.530394\n",
      "  Train results batch 39, loss 0.417413\n",
      "  Train results batch 40, loss 0.432822\n",
      "  Train results batch 41, loss 0.540118\n",
      "  Train results batch 42, loss 0.374406\n",
      "  Train results batch 43, loss 0.432139\n",
      "  Train results batch 44, loss 0.428448\n",
      "  Train results batch 45, loss 0.407462\n",
      "  Train results batch 46, loss 0.451049\n",
      "  Train results batch 47, loss 0.482314\n",
      "  Train results batch 48, loss 0.449668\n",
      "  Train results batch 49, loss 0.541919\n",
      "  Train results batch 50, loss 0.440608\n",
      "  Train results batch 51, loss 0.447969\n",
      "  Train results batch 52, loss 0.426144\n",
      "  Train results batch 53, loss 0.379106\n",
      "  Train results batch 54, loss 0.440017\n",
      "  Train results batch 55, loss 0.375856\n",
      "  Train results batch 56, loss 0.423026\n",
      "  Train results batch 57, loss 0.399277\n",
      "Training results epoch 4, loss 0.000892842327255\n",
      "  Dev results batch 0, loss 0.428878\n",
      "  Dev results batch 1, loss 0.461832\n",
      "  Dev results batch 2, loss 0.397856\n",
      "  Dev results batch 3, loss 0.401513\n",
      "  Dev results batch 4, loss 0.456534\n",
      "  Dev results batch 5, loss 0.370377\n",
      "  Dev results batch 6, loss 0.430745\n",
      "  Dev results batch 7, loss 0.422733\n",
      "  Dev results batch 8, loss 0.434955\n",
      "  Dev results batch 9, loss 0.437094\n",
      "  Dev results batch 10, loss 0.451468\n",
      "  Dev results batch 11, loss 0.419633\n",
      "  Dev results batch 12, loss 0.385089\n",
      "  Dev results batch 13, loss 0.466901\n",
      "  Dev results batch 14, loss 0.516675\n",
      "  Dev results batch 15, loss 0.482652\n",
      "  Dev results batch 16, loss 0.470756\n",
      "  Dev results batch 17, loss 0.456797\n",
      "  Dev results batch 18, loss 0.401936\n",
      "Dev results epoch 4, loss 0.000830355695638\n",
      "  Train results batch 0, loss 0.493982\n",
      "  Train results batch 1, loss 0.397038\n",
      "  Train results batch 2, loss 0.400053\n",
      "  Train results batch 3, loss 0.413985\n",
      "  Train results batch 4, loss 0.35917\n",
      "  Train results batch 5, loss 0.399551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train results batch 6, loss 0.395297\n",
      "  Train results batch 7, loss 0.378347\n",
      "  Train results batch 8, loss 0.356214\n",
      "  Train results batch 9, loss 0.387321\n",
      "  Train results batch 10, loss 0.420265\n",
      "  Train results batch 11, loss 0.401444\n",
      "  Train results batch 12, loss 0.480586\n",
      "  Train results batch 13, loss 0.426355\n",
      "  Train results batch 14, loss 0.416635\n",
      "  Train results batch 15, loss 0.41679\n",
      "  Train results batch 16, loss 0.411181\n",
      "  Train results batch 17, loss 0.3828\n",
      "  Train results batch 18, loss 0.428458\n",
      "  Train results batch 19, loss 0.389428\n",
      "  Train results batch 20, loss 0.387386\n",
      "  Train results batch 21, loss 0.418089\n",
      "  Train results batch 22, loss 0.399487\n",
      "  Train results batch 23, loss 0.349103\n",
      "  Train results batch 24, loss 0.4141\n",
      "  Train results batch 25, loss 0.421721\n",
      "  Train results batch 26, loss 0.41958\n",
      "  Train results batch 27, loss 0.412013\n",
      "  Train results batch 28, loss 0.417829\n",
      "  Train results batch 29, loss 0.337066\n",
      "  Train results batch 30, loss 0.388976\n",
      "  Train results batch 31, loss 0.398887\n",
      "  Train results batch 32, loss 0.455486\n",
      "  Train results batch 33, loss 0.38373\n",
      "  Train results batch 34, loss 0.407693\n",
      "  Train results batch 35, loss 0.467765\n",
      "  Train results batch 36, loss 0.354399\n",
      "  Train results batch 37, loss 0.398177\n",
      "  Train results batch 38, loss 0.396846\n",
      "  Train results batch 39, loss 0.409954\n",
      "  Train results batch 40, loss 0.418678\n",
      "  Train results batch 41, loss 0.38463\n",
      "  Train results batch 42, loss 0.442703\n",
      "  Train results batch 43, loss 0.450876\n",
      "  Train results batch 44, loss 0.411104\n",
      "  Train results batch 45, loss 0.422851\n",
      "  Train results batch 46, loss 0.434685\n",
      "  Train results batch 47, loss 0.419988\n",
      "  Train results batch 48, loss 0.426659\n",
      "  Train results batch 49, loss 0.440116\n",
      "  Train results batch 50, loss 0.397155\n",
      "  Train results batch 51, loss 0.387368\n",
      "  Train results batch 52, loss 0.500832\n",
      "  Train results batch 53, loss 0.38328\n",
      "  Train results batch 54, loss 0.448103\n",
      "  Train results batch 55, loss 0.469455\n",
      "  Train results batch 56, loss 0.367092\n",
      "  Train results batch 57, loss 0.429517\n",
      "Training results epoch 5, loss 0.000795044339275\n",
      "  Dev results batch 0, loss 0.444484\n",
      "  Dev results batch 1, loss 0.443212\n",
      "  Dev results batch 2, loss 0.435076\n",
      "  Dev results batch 3, loss 0.422916\n",
      "  Dev results batch 4, loss 0.474622\n",
      "  Dev results batch 5, loss 0.405597\n",
      "  Dev results batch 6, loss 0.425079\n",
      "  Dev results batch 7, loss 0.428714\n",
      "  Dev results batch 8, loss 0.463225\n",
      "  Dev results batch 9, loss 0.46524\n",
      "  Dev results batch 10, loss 0.458507\n",
      "  Dev results batch 11, loss 0.415319\n",
      "  Dev results batch 12, loss 0.390584\n",
      "  Dev results batch 13, loss 0.453448\n",
      "  Dev results batch 14, loss 0.50065\n",
      "  Dev results batch 15, loss 0.498449\n",
      "  Dev results batch 16, loss 0.499201\n",
      "  Dev results batch 17, loss 0.47398\n",
      "  Dev results batch 18, loss 0.384879\n",
      "Dev results epoch 5, loss 0.000849252137256\n",
      "  Train results batch 0, loss 0.432978\n",
      "  Train results batch 1, loss 0.405567\n",
      "  Train results batch 2, loss 0.544874\n",
      "  Train results batch 3, loss 0.63056\n",
      "  Train results batch 4, loss 0.435118\n",
      "  Train results batch 5, loss 0.433384\n",
      "  Train results batch 6, loss 0.587803\n",
      "  Train results batch 7, loss 0.448816\n",
      "  Train results batch 8, loss 0.400603\n",
      "  Train results batch 9, loss 0.577872\n",
      "  Train results batch 10, loss 0.487668\n",
      "  Train results batch 11, loss 0.376963\n",
      "  Train results batch 12, loss 0.46649\n",
      "  Train results batch 13, loss 0.422921\n",
      "  Train results batch 14, loss 0.42631\n",
      "  Train results batch 15, loss 0.579106\n",
      "  Train results batch 16, loss 0.398946\n",
      "  Train results batch 17, loss 0.51339\n",
      "  Train results batch 18, loss 0.503499\n",
      "  Train results batch 19, loss 0.44374\n",
      "  Train results batch 20, loss 0.43732\n",
      "  Train results batch 21, loss 0.630391\n",
      "  Train results batch 22, loss 0.478154\n",
      "  Train results batch 23, loss 0.444866\n",
      "  Train results batch 24, loss 0.559131\n",
      "  Train results batch 25, loss 0.421147\n",
      "  Train results batch 26, loss 0.485513\n",
      "  Train results batch 27, loss 0.519347\n",
      "  Train results batch 28, loss 0.441316\n",
      "  Train results batch 29, loss 0.404912\n",
      "  Train results batch 30, loss 0.599956\n",
      "  Train results batch 31, loss 0.540405\n",
      "  Train results batch 32, loss 0.395299\n",
      "  Train results batch 33, loss 0.408836\n",
      "  Train results batch 34, loss 0.417017\n",
      "  Train results batch 35, loss 0.418505\n",
      "  Train results batch 36, loss 0.398559\n",
      "  Train results batch 37, loss 0.402166\n",
      "  Train results batch 38, loss 0.38989\n",
      "  Train results batch 39, loss 0.438487\n",
      "  Train results batch 40, loss 0.378057\n",
      "  Train results batch 41, loss 0.359751\n",
      "  Train results batch 42, loss 0.427605\n",
      "  Train results batch 43, loss 0.421341\n",
      "  Train results batch 44, loss 0.461471\n",
      "  Train results batch 45, loss 0.471754\n",
      "  Train results batch 46, loss 0.367464\n",
      "  Train results batch 47, loss 0.442749\n",
      "  Train results batch 48, loss 0.463729\n",
      "  Train results batch 49, loss 0.36516\n",
      "  Train results batch 50, loss 0.439943\n",
      "  Train results batch 51, loss 0.555041\n",
      "  Train results batch 52, loss 0.412002\n",
      "  Train results batch 53, loss 0.472323\n",
      "  Train results batch 54, loss 0.47258\n",
      "  Train results batch 55, loss 0.391787\n",
      "  Train results batch 56, loss 0.390342\n",
      "  Train results batch 57, loss 0.475965\n",
      "Training results epoch 6, loss 0.000884751632627\n",
      "  Dev results batch 0, loss 0.404261\n",
      "  Dev results batch 1, loss 0.434745\n",
      "  Dev results batch 2, loss 0.368592\n",
      "  Dev results batch 3, loss 0.372573\n",
      "  Dev results batch 4, loss 0.414031\n",
      "  Dev results batch 5, loss 0.356451\n",
      "  Dev results batch 6, loss 0.389977\n",
      "  Dev results batch 7, loss 0.387662\n",
      "  Dev results batch 8, loss 0.418429\n",
      "  Dev results batch 9, loss 0.41389\n",
      "  Dev results batch 10, loss 0.425139\n",
      "  Dev results batch 11, loss 0.409891\n",
      "  Dev results batch 12, loss 0.347613\n",
      "  Dev results batch 13, loss 0.422035\n",
      "  Dev results batch 14, loss 0.474987\n",
      "  Dev results batch 15, loss 0.427708\n",
      "  Dev results batch 16, loss 0.439599\n",
      "  Dev results batch 17, loss 0.428373\n",
      "  Dev results batch 18, loss 0.371467\n",
      "Dev results epoch 6, loss 0.000771591002572\n",
      "  Train results batch 0, loss 0.387837\n",
      "  Train results batch 1, loss 0.416203\n",
      "  Train results batch 2, loss 0.39669\n",
      "  Train results batch 3, loss 0.415405\n",
      "  Train results batch 4, loss 0.437446\n",
      "  Train results batch 5, loss 0.467533\n",
      "  Train results batch 6, loss 0.354926\n",
      "  Train results batch 7, loss 0.516186\n",
      "  Train results batch 8, loss 0.612358\n",
      "  Train results batch 9, loss 0.452921\n",
      "  Train results batch 10, loss 0.385051\n",
      "  Train results batch 11, loss 0.410479\n",
      "  Train results batch 12, loss 0.446885\n",
      "  Train results batch 13, loss 0.415983\n",
      "  Train results batch 14, loss 0.452322\n",
      "  Train results batch 15, loss 0.460739\n",
      "  Train results batch 16, loss 0.40336\n",
      "  Train results batch 17, loss 0.465746\n",
      "  Train results batch 18, loss 0.443505\n",
      "  Train results batch 19, loss 0.390867\n",
      "  Train results batch 20, loss 0.499548\n",
      "  Train results batch 21, loss 0.436224\n",
      "  Train results batch 22, loss 0.43545\n",
      "  Train results batch 23, loss 0.377465\n",
      "  Train results batch 24, loss 0.367277\n",
      "  Train results batch 25, loss 0.39269\n",
      "  Train results batch 26, loss 0.343945\n",
      "  Train results batch 27, loss 0.425028\n",
      "  Train results batch 28, loss 0.35686\n",
      "  Train results batch 29, loss 0.422031\n",
      "  Train results batch 30, loss 0.434049\n",
      "  Train results batch 31, loss 0.459196\n",
      "  Train results batch 32, loss 0.357266\n",
      "  Train results batch 33, loss 0.45977\n",
      "  Train results batch 34, loss 0.425905\n",
      "  Train results batch 35, loss 0.392289\n",
      "  Train results batch 36, loss 0.524346\n",
      "  Train results batch 37, loss 0.449608\n",
      "  Train results batch 38, loss 0.374101\n",
      "  Train results batch 39, loss 0.519261\n",
      "  Train results batch 40, loss 0.544918\n",
      "  Train results batch 41, loss 0.383034\n",
      "  Train results batch 42, loss 0.49701\n",
      "  Train results batch 43, loss 0.4026\n",
      "  Train results batch 44, loss 0.407856\n",
      "  Train results batch 45, loss 0.515756\n",
      "  Train results batch 46, loss 0.528112\n",
      "  Train results batch 47, loss 0.457465\n",
      "  Train results batch 48, loss 0.509774\n",
      "  Train results batch 49, loss 0.444287\n",
      "  Train results batch 50, loss 0.436447\n",
      "  Train results batch 51, loss 0.429449\n",
      "  Train results batch 52, loss 0.460443\n",
      "  Train results batch 53, loss 0.452517\n",
      "  Train results batch 54, loss 0.424204\n",
      "  Train results batch 55, loss 0.440761\n",
      "  Train results batch 56, loss 0.476851\n",
      "  Train results batch 57, loss 0.355725\n",
      "Training results epoch 7, loss 0.000845882896069\n",
      "  Dev results batch 0, loss 0.497471\n",
      "  Dev results batch 1, loss 0.450439\n",
      "  Dev results batch 2, loss 0.473875\n",
      "  Dev results batch 3, loss 0.466087\n",
      "  Dev results batch 4, loss 0.500049\n",
      "  Dev results batch 5, loss 0.451425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Dev results batch 6, loss 0.433113\n",
      "  Dev results batch 7, loss 0.453265\n",
      "  Dev results batch 8, loss 0.458946\n",
      "  Dev results batch 9, loss 0.473936\n",
      "  Dev results batch 10, loss 0.479998\n",
      "  Dev results batch 11, loss 0.44028\n",
      "  Dev results batch 12, loss 0.389614\n",
      "  Dev results batch 13, loss 0.472066\n",
      "  Dev results batch 14, loss 0.516823\n",
      "  Dev results batch 15, loss 0.505862\n",
      "  Dev results batch 16, loss 0.48869\n",
      "  Dev results batch 17, loss 0.452845\n",
      "  Dev results batch 18, loss 0.407077\n",
      "Dev results epoch 7, loss 0.000882156541582\n",
      "  Train results batch 0, loss 0.452698\n",
      "  Train results batch 1, loss 0.424393\n",
      "  Train results batch 2, loss 0.479554\n",
      "  Train results batch 3, loss 0.442627\n",
      "  Train results batch 4, loss 0.454932\n",
      "  Train results batch 5, loss 0.387547\n",
      "  Train results batch 6, loss 0.374886\n",
      "  Train results batch 7, loss 0.47603\n",
      "  Train results batch 8, loss 0.528676\n",
      "  Train results batch 9, loss 0.397058\n",
      "  Train results batch 10, loss 0.389932\n",
      "  Train results batch 11, loss 0.489365\n",
      "  Train results batch 12, loss 0.426681\n",
      "  Train results batch 13, loss 0.404255\n",
      "  Train results batch 14, loss 0.445724\n",
      "  Train results batch 15, loss 0.39621\n",
      "  Train results batch 16, loss 0.349308\n",
      "  Train results batch 17, loss 0.391138\n",
      "  Train results batch 18, loss 0.391459\n",
      "  Train results batch 19, loss 0.408726\n",
      "  Train results batch 20, loss 0.38065\n",
      "  Train results batch 21, loss 0.370246\n",
      "  Train results batch 22, loss 0.360651\n",
      "  Train results batch 23, loss 0.348899\n",
      "  Train results batch 24, loss 0.363645\n",
      "  Train results batch 25, loss 0.444934\n",
      "  Train results batch 26, loss 0.387826\n",
      "  Train results batch 27, loss 0.332025\n",
      "  Train results batch 28, loss 0.416664\n",
      "  Train results batch 29, loss 0.359235\n",
      "  Train results batch 30, loss 0.372974\n",
      "  Train results batch 31, loss 0.593911\n",
      "  Train results batch 32, loss 0.659814\n",
      "  Train results batch 33, loss 0.383048\n",
      "  Train results batch 34, loss 0.548292\n",
      "  Train results batch 35, loss 0.917158\n",
      "  Train results batch 36, loss 0.769255\n",
      "  Train results batch 37, loss 0.473862\n",
      "  Train results batch 38, loss 0.470599\n",
      "  Train results batch 39, loss 0.744651\n",
      "  Train results batch 40, loss 0.848407\n",
      "  Train results batch 41, loss 0.387086\n",
      "  Train results batch 42, loss 0.718591\n",
      "  Train results batch 43, loss 0.838713\n",
      "  Train results batch 44, loss 0.386313\n",
      "  Train results batch 45, loss 0.760464\n",
      "  Train results batch 46, loss 0.736775\n",
      "  Train results batch 47, loss 0.355297\n",
      "  Train results batch 48, loss 0.633368\n",
      "  Train results batch 49, loss 0.640262\n",
      "  Train results batch 50, loss 0.405212\n",
      "  Train results batch 51, loss 0.508836\n",
      "  Train results batch 52, loss 0.583456\n",
      "  Train results batch 53, loss 0.478123\n",
      "  Train results batch 54, loss 0.404658\n",
      "  Train results batch 55, loss 0.422948\n",
      "  Train results batch 56, loss 0.358192\n",
      "  Train results batch 57, loss 0.448744\n",
      "Training results epoch 8, loss 0.000938406624731\n",
      "  Dev results batch 0, loss 0.43065\n",
      "  Dev results batch 1, loss 0.418153\n",
      "  Dev results batch 2, loss 0.414717\n",
      "  Dev results batch 3, loss 0.404339\n",
      "  Dev results batch 4, loss 0.458296\n",
      "  Dev results batch 5, loss 0.390016\n",
      "  Dev results batch 6, loss 0.409334\n",
      "  Dev results batch 7, loss 0.408471\n",
      "  Dev results batch 8, loss 0.426095\n",
      "  Dev results batch 9, loss 0.439564\n",
      "  Dev results batch 10, loss 0.437611\n",
      "  Dev results batch 11, loss 0.395927\n",
      "  Dev results batch 12, loss 0.369164\n",
      "  Dev results batch 13, loss 0.431465\n",
      "  Dev results batch 14, loss 0.475433\n",
      "  Dev results batch 15, loss 0.472606\n",
      "  Dev results batch 16, loss 0.4647\n",
      "  Dev results batch 17, loss 0.43488\n",
      "  Dev results batch 18, loss 0.365033\n",
      "Dev results epoch 8, loss 0.000805531536594\n",
      "  Train results batch 0, loss 0.361293\n",
      "  Train results batch 1, loss 0.433952\n",
      "  Train results batch 2, loss 0.433463\n",
      "  Train results batch 3, loss 0.418541\n",
      "  Train results batch 4, loss 0.409099\n",
      "  Train results batch 5, loss 0.419819\n",
      "  Train results batch 6, loss 0.395096\n",
      "  Train results batch 7, loss 0.453406\n",
      "  Train results batch 8, loss 0.418529\n",
      "  Train results batch 9, loss 0.429674\n",
      "  Train results batch 10, loss 0.424995\n",
      "  Train results batch 11, loss 0.402486\n",
      "  Train results batch 12, loss 0.441108\n",
      "  Train results batch 13, loss 0.414284\n",
      "  Train results batch 14, loss 0.389193\n",
      "  Train results batch 15, loss 0.366781\n",
      "  Train results batch 16, loss 0.387158\n",
      "  Train results batch 17, loss 0.367728\n",
      "  Train results batch 18, loss 0.428237\n",
      "  Train results batch 19, loss 0.425848\n",
      "  Train results batch 20, loss 0.401704\n",
      "  Train results batch 21, loss 0.406883\n",
      "  Train results batch 22, loss 0.401733\n",
      "  Train results batch 23, loss 0.397219\n",
      "  Train results batch 24, loss 0.38779\n",
      "  Train results batch 25, loss 0.392505\n",
      "  Train results batch 26, loss 0.430424\n",
      "  Train results batch 27, loss 0.402061\n",
      "  Train results batch 28, loss 0.406043\n",
      "  Train results batch 29, loss 0.384153\n",
      "  Train results batch 30, loss 0.494594\n",
      "  Train results batch 31, loss 0.510448\n",
      "  Train results batch 32, loss 0.411593\n",
      "  Train results batch 33, loss 0.469492\n",
      "  Train results batch 34, loss 0.4686\n",
      "  Train results batch 35, loss 0.405969\n",
      "  Train results batch 36, loss 0.368394\n",
      "  Train results batch 37, loss 0.45983\n",
      "  Train results batch 38, loss 0.395256\n",
      "  Train results batch 39, loss 0.348194\n",
      "  Train results batch 40, loss 0.466283\n",
      "  Train results batch 41, loss 0.499874\n",
      "  Train results batch 42, loss 0.396028\n",
      "  Train results batch 43, loss 0.431631\n",
      "  Train results batch 44, loss 0.41772\n",
      "  Train results batch 45, loss 0.461722\n",
      "  Train results batch 46, loss 0.435507\n",
      "  Train results batch 47, loss 0.464209\n",
      "  Train results batch 48, loss 0.390808\n",
      "  Train results batch 49, loss 0.44055\n",
      "  Train results batch 50, loss 0.491637\n",
      "  Train results batch 51, loss 0.368765\n",
      "  Train results batch 52, loss 0.401517\n",
      "  Train results batch 53, loss 0.402931\n",
      "  Train results batch 54, loss 0.359531\n",
      "  Train results batch 55, loss 0.323176\n",
      "  Train results batch 56, loss 0.366752\n",
      "  Train results batch 57, loss 0.406703\n",
      "Training results epoch 9, loss 0.000803740955458\n",
      "  Dev results batch 0, loss 0.396496\n",
      "  Dev results batch 1, loss 0.407093\n",
      "  Dev results batch 2, loss 0.357431\n",
      "  Dev results batch 3, loss 0.365355\n",
      "  Dev results batch 4, loss 0.399078\n",
      "  Dev results batch 5, loss 0.352386\n",
      "  Dev results batch 6, loss 0.357053\n",
      "  Dev results batch 7, loss 0.374896\n",
      "  Dev results batch 8, loss 0.394389\n",
      "  Dev results batch 9, loss 0.384541\n",
      "  Dev results batch 10, loss 0.401143\n",
      "  Dev results batch 11, loss 0.390733\n",
      "  Dev results batch 12, loss 0.33047\n",
      "  Dev results batch 13, loss 0.397969\n",
      "  Dev results batch 14, loss 0.444218\n",
      "  Dev results batch 15, loss 0.402739\n",
      "  Dev results batch 16, loss 0.41392\n",
      "  Dev results batch 17, loss 0.393258\n",
      "  Dev results batch 18, loss 0.361994\n",
      "Dev results epoch 9, loss 0.000733322693381\n",
      "  Train results batch 0, loss 0.410846\n",
      "  Train results batch 1, loss 0.478417\n",
      "  Train results batch 2, loss 0.528718\n",
      "  Train results batch 3, loss 0.491707\n",
      "  Train results batch 4, loss 0.382077\n",
      "  Train results batch 5, loss 0.35165\n",
      "  Train results batch 6, loss 0.428339\n",
      "  Train results batch 7, loss 0.360695\n",
      "  Train results batch 8, loss 0.380415\n",
      "  Train results batch 9, loss 0.407965\n",
      "  Train results batch 10, loss 0.380685\n",
      "  Train results batch 11, loss 0.509363\n",
      "  Train results batch 12, loss 0.494517\n",
      "  Train results batch 13, loss 0.390467\n",
      "  Train results batch 14, loss 0.445975\n",
      "  Train results batch 15, loss 0.516195\n",
      "  Train results batch 16, loss 0.388327\n",
      "  Train results batch 17, loss 0.3847\n",
      "  Train results batch 18, loss 0.377132\n",
      "  Train results batch 19, loss 0.385673\n",
      "  Train results batch 20, loss 0.347968\n",
      "  Train results batch 21, loss 0.431312\n",
      "  Train results batch 22, loss 0.39825\n",
      "  Train results batch 23, loss 0.386137\n",
      "  Train results batch 24, loss 0.39508\n",
      "  Train results batch 25, loss 0.479767\n",
      "  Train results batch 26, loss 0.385016\n",
      "  Train results batch 27, loss 0.397388\n",
      "  Train results batch 28, loss 0.392154\n",
      "  Train results batch 29, loss 0.409922\n",
      "  Train results batch 30, loss 0.461327\n",
      "  Train results batch 31, loss 0.389165\n",
      "  Train results batch 32, loss 0.366499\n",
      "  Train results batch 33, loss 0.42145\n",
      "  Train results batch 34, loss 0.380713\n",
      "  Train results batch 35, loss 0.450717\n",
      "  Train results batch 36, loss 0.425188\n",
      "  Train results batch 37, loss 0.478901\n",
      "  Train results batch 38, loss 0.393398\n",
      "  Train results batch 39, loss 0.42792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train results batch 40, loss 0.491933\n",
      "  Train results batch 41, loss 0.38102\n",
      "  Train results batch 42, loss 0.469212\n",
      "  Train results batch 43, loss 0.486724\n",
      "  Train results batch 44, loss 0.37486\n",
      "  Train results batch 45, loss 0.384591\n",
      "  Train results batch 46, loss 0.412796\n",
      "  Train results batch 47, loss 0.381522\n",
      "  Train results batch 48, loss 0.438613\n",
      "  Train results batch 49, loss 0.351587\n",
      "  Train results batch 50, loss 0.497761\n",
      "  Train results batch 51, loss 0.547874\n",
      "  Train results batch 52, loss 0.394701\n",
      "  Train results batch 53, loss 0.378122\n",
      "  Train results batch 54, loss 0.435158\n",
      "  Train results batch 55, loss 0.434772\n",
      "  Train results batch 56, loss 0.372886\n",
      "  Train results batch 57, loss 0.401503\n",
      "Training results epoch 10, loss 0.000812377733319\n",
      "  Dev results batch 0, loss 0.541698\n",
      "  Dev results batch 1, loss 0.496569\n",
      "  Dev results batch 2, loss 0.548289\n",
      "  Dev results batch 3, loss 0.520897\n",
      "  Dev results batch 4, loss 0.583666\n",
      "  Dev results batch 5, loss 0.514405\n",
      "  Dev results batch 6, loss 0.528646\n",
      "  Dev results batch 7, loss 0.535318\n",
      "  Dev results batch 8, loss 0.566919\n",
      "  Dev results batch 9, loss 0.570698\n",
      "  Dev results batch 10, loss 0.55019\n",
      "  Dev results batch 11, loss 0.494357\n",
      "  Dev results batch 12, loss 0.4807\n",
      "  Dev results batch 13, loss 0.535696\n",
      "  Dev results batch 14, loss 0.582249\n",
      "  Dev results batch 15, loss 0.585866\n",
      "  Dev results batch 16, loss 0.596818\n",
      "  Dev results batch 17, loss 0.561843\n",
      "  Dev results batch 18, loss 0.464881\n",
      "Dev results epoch 10, loss 0.00102710023945\n",
      "  Train results batch 0, loss 0.576215\n",
      "  Train results batch 1, loss 0.536812\n",
      "  Train results batch 2, loss 0.408831\n",
      "  Train results batch 3, loss 0.51341\n",
      "  Train results batch 4, loss 0.602316\n",
      "  Train results batch 5, loss 0.424277\n",
      "  Train results batch 6, loss 0.476262\n",
      "  Train results batch 7, loss 0.915955\n",
      "  Train results batch 8, loss 0.852375\n",
      "  Train results batch 9, loss 0.384034\n",
      "  Train results batch 10, loss 0.503711\n",
      "  Train results batch 11, loss 0.715762\n",
      "  Train results batch 12, loss 0.556215\n",
      "  Train results batch 13, loss 0.348095\n",
      "  Train results batch 14, loss 0.506913\n",
      "  Train results batch 15, loss 0.670902\n",
      "  Train results batch 16, loss 0.451756\n",
      "  Train results batch 17, loss 0.434221\n",
      "  Train results batch 18, loss 0.544849\n",
      "  Train results batch 19, loss 0.549323\n",
      "  Train results batch 20, loss 0.331548\n",
      "  Train results batch 21, loss 0.465753\n",
      "  Train results batch 22, loss 0.582712\n",
      "  Train results batch 23, loss 0.477734\n",
      "  Train results batch 24, loss 0.372204\n",
      "  Train results batch 25, loss 0.572395\n",
      "  Train results batch 26, loss 0.516906\n",
      "  Train results batch 27, loss 0.348826\n",
      "  Train results batch 28, loss 0.4474\n",
      "  Train results batch 29, loss 0.583765\n",
      "  Train results batch 30, loss 0.519728\n",
      "  Train results batch 31, loss 0.363013\n",
      "  Train results batch 32, loss 0.615788\n",
      "  Train results batch 33, loss 0.555852\n",
      "  Train results batch 34, loss 0.414247\n",
      "  Train results batch 35, loss 0.647941\n",
      "  Train results batch 36, loss 0.572889\n",
      "  Train results batch 37, loss 0.373885\n",
      "  Train results batch 38, loss 0.443084\n",
      "  Train results batch 39, loss 0.422656\n",
      "  Train results batch 40, loss 0.349062\n",
      "  Train results batch 41, loss 0.395226\n",
      "  Train results batch 42, loss 0.468644\n",
      "  Train results batch 43, loss 0.420604\n",
      "  Train results batch 44, loss 0.386748\n",
      "  Train results batch 45, loss 0.385084\n",
      "  Train results batch 46, loss 0.358828\n",
      "  Train results batch 47, loss 0.363667\n",
      "  Train results batch 48, loss 0.37619\n",
      "  Train results batch 49, loss 0.418462\n",
      "  Train results batch 50, loss 0.361762\n",
      "  Train results batch 51, loss 0.375389\n",
      "  Train results batch 52, loss 0.425268\n",
      "  Train results batch 53, loss 0.432819\n",
      "  Train results batch 54, loss 0.378257\n",
      "  Train results batch 55, loss 0.41298\n",
      "  Train results batch 56, loss 0.569552\n",
      "  Train results batch 57, loss 0.514224\n",
      "Training results epoch 11, loss 0.000934013759826\n",
      "  Dev results batch 0, loss 0.411212\n",
      "  Dev results batch 1, loss 0.41139\n",
      "  Dev results batch 2, loss 0.369899\n",
      "  Dev results batch 3, loss 0.375148\n",
      "  Dev results batch 4, loss 0.405398\n",
      "  Dev results batch 5, loss 0.371794\n",
      "  Dev results batch 6, loss 0.358511\n",
      "  Dev results batch 7, loss 0.382325\n",
      "  Dev results batch 8, loss 0.399563\n",
      "  Dev results batch 9, loss 0.393118\n",
      "  Dev results batch 10, loss 0.405511\n",
      "  Dev results batch 11, loss 0.398019\n",
      "  Dev results batch 12, loss 0.336948\n",
      "  Dev results batch 13, loss 0.397562\n",
      "  Dev results batch 14, loss 0.447353\n",
      "  Dev results batch 15, loss 0.406429\n",
      "  Dev results batch 16, loss 0.418254\n",
      "  Dev results batch 17, loss 0.391343\n",
      "  Dev results batch 18, loss 0.367156\n",
      "Dev results epoch 11, loss 0.000745513349912\n",
      "  Train results batch 0, loss 0.3805\n",
      "  Train results batch 1, loss 0.443662\n",
      "  Train results batch 2, loss 0.584599\n",
      "  Train results batch 3, loss 0.52867\n",
      "  Train results batch 4, loss 0.407697\n",
      "  Train results batch 5, loss 0.436642\n",
      "  Train results batch 6, loss 0.365001\n",
      "  Train results batch 7, loss 0.401671\n",
      "  Train results batch 8, loss 0.57704\n",
      "  Train results batch 9, loss 0.454255\n",
      "  Train results batch 10, loss 0.327522\n",
      "  Train results batch 11, loss 0.451752\n",
      "  Train results batch 12, loss 0.36778\n",
      "  Train results batch 13, loss 0.392775\n",
      "  Train results batch 14, loss 0.374568\n",
      "  Train results batch 15, loss 0.386846\n",
      "  Train results batch 16, loss 0.395376\n",
      "  Train results batch 17, loss 0.367966\n",
      "  Train results batch 18, loss 0.371874\n",
      "  Train results batch 19, loss 0.392193\n",
      "  Train results batch 20, loss 0.364929\n",
      "  Train results batch 21, loss 0.360019\n",
      "  Train results batch 22, loss 0.392388\n",
      "  Train results batch 23, loss 0.393849\n",
      "  Train results batch 24, loss 0.478413\n",
      "  Train results batch 25, loss 0.473525\n",
      "  Train results batch 26, loss 0.389004\n",
      "  Train results batch 27, loss 0.531079\n",
      "  Train results batch 28, loss 0.722105\n",
      "  Train results batch 29, loss 0.694658\n",
      "  Train results batch 30, loss 0.35906\n",
      "  Train results batch 31, loss 0.588154\n",
      "  Train results batch 32, loss 0.799732\n",
      "  Train results batch 33, loss 0.521107\n",
      "  Train results batch 34, loss 0.396823\n",
      "  Train results batch 35, loss 0.734113\n",
      "  Train results batch 36, loss 1.05156\n",
      "  Train results batch 37, loss 0.729287\n",
      "  Train results batch 38, loss 0.582337\n",
      "  Train results batch 39, loss 1.77941\n",
      "  Train results batch 40, loss 1.4701\n",
      "  Train results batch 41, loss 0.40398\n",
      "  Train results batch 42, loss 1.11517\n",
      "  Train results batch 43, loss 1.46156\n",
      "  Train results batch 44, loss 0.504427\n",
      "  Train results batch 45, loss 0.97213\n",
      "  Train results batch 46, loss 1.55634\n",
      "  Train results batch 47, loss 0.695325\n",
      "  Train results batch 48, loss 0.451825\n",
      "  Train results batch 49, loss 0.923414\n",
      "  Train results batch 50, loss 0.512896\n",
      "  Train results batch 51, loss 0.650926\n",
      "  Train results batch 52, loss 1.0188\n",
      "  Train results batch 53, loss 0.57594\n",
      "  Train results batch 54, loss 0.455575\n",
      "  Train results batch 55, loss 0.639784\n",
      "  Train results batch 56, loss 0.459332\n",
      "  Train results batch 57, loss 0.495128\n",
      "Training results epoch 12, loss 0.00117155220339\n",
      "  Dev results batch 0, loss 0.717458\n",
      "  Dev results batch 1, loss 0.667371\n",
      "  Dev results batch 2, loss 0.749861\n",
      "  Dev results batch 3, loss 0.704514\n",
      "  Dev results batch 4, loss 0.828965\n",
      "  Dev results batch 5, loss 0.689539\n",
      "  Dev results batch 6, loss 0.757155\n",
      "  Dev results batch 7, loss 0.743701\n",
      "  Dev results batch 8, loss 0.733027\n",
      "  Dev results batch 9, loss 0.75883\n",
      "  Dev results batch 10, loss 0.739784\n",
      "  Dev results batch 11, loss 0.659396\n",
      "  Dev results batch 12, loss 0.660993\n",
      "  Dev results batch 13, loss 0.742198\n",
      "  Dev results batch 14, loss 0.779588\n",
      "  Dev results batch 15, loss 0.803018\n",
      "  Dev results batch 16, loss 0.762123\n",
      "  Dev results batch 17, loss 0.744266\n",
      "  Dev results batch 18, loss 0.632359\n",
      "Dev results epoch 12, loss 0.00138894253062\n",
      "  Train results batch 0, loss 0.703578\n",
      "  Train results batch 1, loss 0.355478\n",
      "  Train results batch 2, loss 0.436934\n",
      "  Train results batch 3, loss 0.476396\n",
      "  Train results batch 4, loss 0.391514\n",
      "  Train results batch 5, loss 0.417348\n",
      "  Train results batch 6, loss 0.402965\n",
      "  Train results batch 7, loss 0.486367\n",
      "  Train results batch 8, loss 0.527695\n",
      "  Train results batch 9, loss 0.39819\n",
      "  Train results batch 10, loss 0.427617\n",
      "  Train results batch 11, loss 0.546148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train results batch 12, loss 0.380815\n",
      "  Train results batch 13, loss 0.661096\n",
      "  Train results batch 14, loss 0.775604\n",
      "  Train results batch 15, loss 0.461807\n",
      "  Train results batch 16, loss 0.496363\n",
      "  Train results batch 17, loss 0.700317\n",
      "  Train results batch 18, loss 0.38121\n",
      "  Train results batch 19, loss 0.576233\n",
      "  Train results batch 20, loss 0.729822\n",
      "  Train results batch 21, loss 0.400877\n",
      "  Train results batch 22, loss 0.629935\n",
      "  Train results batch 23, loss 0.620508\n",
      "  Train results batch 24, loss 0.44061\n",
      "  Train results batch 25, loss 0.97519\n",
      "  Train results batch 26, loss 0.90961\n",
      "  Train results batch 27, loss 0.400017\n",
      "  Train results batch 28, loss 0.628799\n",
      "  Train results batch 29, loss 0.661312\n",
      "  Train results batch 30, loss 0.493226\n",
      "  Train results batch 31, loss 0.500193\n",
      "  Train results batch 32, loss 0.644779\n",
      "  Train results batch 33, loss 0.40652\n",
      "  Train results batch 34, loss 0.544628\n",
      "  Train results batch 35, loss 0.574734\n",
      "  Train results batch 36, loss 0.419392\n",
      "  Train results batch 37, loss 0.559742\n",
      "  Train results batch 38, loss 0.510984\n",
      "  Train results batch 39, loss 0.464481\n",
      "  Train results batch 40, loss 0.668752\n",
      "  Train results batch 41, loss 0.429995\n",
      "  Train results batch 42, loss 0.474492\n",
      "  Train results batch 43, loss 0.515044\n",
      "  Train results batch 44, loss 0.453256\n",
      "  Train results batch 45, loss 0.485758\n",
      "  Train results batch 46, loss 0.360809\n",
      "  Train results batch 47, loss 0.531211\n",
      "  Train results batch 48, loss 0.466464\n",
      "  Train results batch 49, loss 0.34035\n",
      "  Train results batch 50, loss 0.542771\n",
      "  Train results batch 51, loss 0.428439\n",
      "  Train results batch 52, loss 0.4409\n",
      "  Train results batch 53, loss 0.50496\n",
      "  Train results batch 54, loss 0.437678\n",
      "  Train results batch 55, loss 0.413253\n",
      "  Train results batch 56, loss 0.512437\n",
      "  Train results batch 57, loss 0.338433\n",
      "Training results epoch 13, loss 0.000996431126927\n",
      "  Dev results batch 0, loss 0.502055\n",
      "  Dev results batch 1, loss 0.553821\n",
      "  Dev results batch 2, loss 0.455387\n",
      "  Dev results batch 3, loss 0.480058\n",
      "  Dev results batch 4, loss 0.50705\n",
      "  Dev results batch 5, loss 0.460788\n",
      "  Dev results batch 6, loss 0.462948\n",
      "  Dev results batch 7, loss 0.496307\n",
      "  Dev results batch 8, loss 0.525469\n",
      "  Dev results batch 9, loss 0.489901\n",
      "  Dev results batch 10, loss 0.518497\n",
      "  Dev results batch 11, loss 0.539043\n",
      "  Dev results batch 12, loss 0.450539\n",
      "  Dev results batch 13, loss 0.516479\n",
      "  Dev results batch 14, loss 0.569525\n",
      "  Dev results batch 15, loss 0.492903\n",
      "  Dev results batch 16, loss 0.53284\n",
      "  Dev results batch 17, loss 0.528315\n",
      "  Dev results batch 18, loss 0.502011\n",
      "Dev results epoch 13, loss 0.000959448961658\n",
      "  Train results batch 0, loss 0.501456\n",
      "  Train results batch 1, loss 0.435961\n",
      "  Train results batch 2, loss 0.363709\n",
      "  Train results batch 3, loss 0.53743\n",
      "  Train results batch 4, loss 0.440632\n",
      "  Train results batch 5, loss 0.453941\n",
      "  Train results batch 6, loss 0.616288\n",
      "  Train results batch 7, loss 0.401067\n",
      "  Train results batch 8, loss 0.509676\n",
      "  Train results batch 9, loss 0.525331\n",
      "  Train results batch 10, loss 0.35561\n",
      "  Train results batch 11, loss 0.511307\n",
      "  Train results batch 12, loss 0.473662\n",
      "  Train results batch 13, loss 0.34895\n",
      "  Train results batch 14, loss 0.366848\n",
      "  Train results batch 15, loss 0.40504\n",
      "  Train results batch 16, loss 0.374929\n",
      "  Train results batch 17, loss 0.394743\n",
      "  Train results batch 18, loss 0.428518\n",
      "  Train results batch 19, loss 0.359173\n",
      "  Train results batch 20, loss 0.416398\n",
      "  Train results batch 21, loss 0.399069\n",
      "  Train results batch 22, loss 0.343716\n",
      "  Train results batch 23, loss 0.3604\n",
      "  Train results batch 24, loss 0.428933\n",
      "  Train results batch 25, loss 0.413083\n",
      "  Train results batch 26, loss 0.347638\n",
      "  Train results batch 27, loss 0.372924\n",
      "  Train results batch 28, loss 0.366024\n",
      "  Train results batch 29, loss 0.427739\n",
      "  Train results batch 30, loss 0.476611\n",
      "  Train results batch 31, loss 0.362581\n",
      "  Train results batch 32, loss 0.506881\n",
      "  Train results batch 33, loss 0.369914\n",
      "  Train results batch 34, loss 0.374957\n",
      "  Train results batch 35, loss 0.31254\n",
      "  Train results batch 36, loss 0.426079\n",
      "  Train results batch 37, loss 0.376169\n",
      "  Train results batch 38, loss 0.438011\n",
      "  Train results batch 39, loss 0.356466\n",
      "  Train results batch 40, loss 0.416526\n",
      "  Train results batch 41, loss 0.416983\n",
      "  Train results batch 42, loss 0.368403\n",
      "  Train results batch 43, loss 0.359198\n",
      "  Train results batch 44, loss 0.391177\n",
      "  Train results batch 45, loss 0.437252\n",
      "  Train results batch 46, loss 0.409431\n",
      "  Train results batch 47, loss 0.40494\n",
      "  Train results batch 48, loss 0.427075\n",
      "  Train results batch 49, loss 0.369134\n",
      "  Train results batch 50, loss 0.375309\n",
      "  Train results batch 51, loss 0.384892\n",
      "  Train results batch 52, loss 0.339185\n",
      "  Train results batch 53, loss 0.409201\n",
      "  Train results batch 54, loss 0.356407\n",
      "  Train results batch 55, loss 0.422397\n",
      "  Train results batch 56, loss 0.442166\n",
      "  Train results batch 57, loss 0.343489\n",
      "Training results epoch 14, loss 0.000792551715231\n",
      "  Dev results batch 0, loss 0.41079\n",
      "  Dev results batch 1, loss 0.390692\n",
      "  Dev results batch 2, loss 0.373169\n",
      "  Dev results batch 3, loss 0.370813\n",
      "  Dev results batch 4, loss 0.410977\n",
      "  Dev results batch 5, loss 0.365639\n",
      "  Dev results batch 6, loss 0.371059\n",
      "  Dev results batch 7, loss 0.376942\n",
      "  Dev results batch 8, loss 0.401634\n",
      "  Dev results batch 9, loss 0.400201\n",
      "  Dev results batch 10, loss 0.405101\n",
      "  Dev results batch 11, loss 0.38635\n",
      "  Dev results batch 12, loss 0.331135\n",
      "  Dev results batch 13, loss 0.400934\n",
      "  Dev results batch 14, loss 0.442466\n",
      "  Dev results batch 15, loss 0.413336\n",
      "  Dev results batch 16, loss 0.415571\n",
      "  Dev results batch 17, loss 0.381189\n",
      "  Dev results batch 18, loss 0.353671\n",
      "Dev results epoch 14, loss 0.000740981750009\n",
      "  Train results batch 0, loss 0.392308\n",
      "  Train results batch 1, loss 0.391909\n",
      "  Train results batch 2, loss 0.373706\n",
      "  Train results batch 3, loss 0.389598\n",
      "  Train results batch 4, loss 0.362121\n",
      "  Train results batch 5, loss 0.398366\n",
      "  Train results batch 6, loss 0.353367\n",
      "  Train results batch 7, loss 0.361369\n",
      "  Train results batch 8, loss 0.383874\n",
      "  Train results batch 9, loss 0.403462\n",
      "  Train results batch 10, loss 0.343459\n",
      "  Train results batch 11, loss 0.352423\n",
      "  Train results batch 12, loss 0.370448\n",
      "  Train results batch 13, loss 0.366072\n",
      "  Train results batch 14, loss 0.384497\n",
      "  Train results batch 15, loss 0.353702\n",
      "  Train results batch 16, loss 0.368028\n",
      "  Train results batch 17, loss 0.424166\n",
      "  Train results batch 18, loss 0.416147\n",
      "  Train results batch 19, loss 0.415336\n",
      "  Train results batch 20, loss 0.32746\n",
      "  Train results batch 21, loss 0.402976\n",
      "  Train results batch 22, loss 0.384101\n",
      "  Train results batch 23, loss 0.344766\n",
      "  Train results batch 24, loss 0.410383\n",
      "  Train results batch 25, loss 0.379667\n",
      "  Train results batch 26, loss 0.367207\n",
      "  Train results batch 27, loss 0.453313\n",
      "  Train results batch 28, loss 0.387868\n",
      "  Train results batch 29, loss 0.459379\n",
      "  Train results batch 30, loss 0.393147\n",
      "  Train results batch 31, loss 0.495911\n",
      "  Train results batch 32, loss 0.384119\n",
      "  Train results batch 33, loss 0.351665\n",
      "  Train results batch 34, loss 0.465194\n",
      "  Train results batch 35, loss 0.461427\n",
      "  Train results batch 36, loss 0.352432\n",
      "  Train results batch 37, loss 0.525629\n",
      "  Train results batch 38, loss 0.464014\n",
      "  Train results batch 39, loss 0.396036\n",
      "  Train results batch 40, loss 0.547925\n",
      "  Train results batch 41, loss 0.469804\n",
      "  Train results batch 42, loss 0.394937\n",
      "  Train results batch 43, loss 0.585816\n",
      "  Train results batch 44, loss 0.48523\n",
      "  Train results batch 45, loss 0.3862\n",
      "  Train results batch 46, loss 0.719796\n",
      "  Train results batch 47, loss 0.589243\n",
      "  Train results batch 48, loss 0.402782\n",
      "  Train results batch 49, loss 0.590184\n",
      "  Train results batch 50, loss 0.455738\n",
      "  Train results batch 51, loss 0.464916\n",
      "  Train results batch 52, loss 0.615826\n",
      "  Train results batch 53, loss 0.508956\n",
      "  Train results batch 54, loss 0.355155\n",
      "  Train results batch 55, loss 0.414606\n",
      "  Train results batch 56, loss 0.363941\n",
      "  Train results batch 57, loss 0.420564\n",
      "Training results epoch 15, loss 0.000820215332437\n",
      "  Dev results batch 0, loss 0.524832\n",
      "  Dev results batch 1, loss 0.500022\n",
      "  Dev results batch 2, loss 0.53056\n",
      "  Dev results batch 3, loss 0.497435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Dev results batch 4, loss 0.598256\n",
      "  Dev results batch 5, loss 0.496413\n",
      "  Dev results batch 6, loss 0.524085\n",
      "  Dev results batch 7, loss 0.51534\n",
      "  Dev results batch 8, loss 0.520979\n",
      "  Dev results batch 9, loss 0.540719\n",
      "  Dev results batch 10, loss 0.528674\n",
      "  Dev results batch 11, loss 0.471635\n",
      "  Dev results batch 12, loss 0.470365\n",
      "  Dev results batch 13, loss 0.53413\n",
      "  Dev results batch 14, loss 0.558848\n",
      "  Dev results batch 15, loss 0.573649\n",
      "  Dev results batch 16, loss 0.537475\n",
      "  Dev results batch 17, loss 0.523115\n",
      "  Dev results batch 18, loss 0.457577\n",
      "Dev results epoch 15, loss 0.0009915017189\n",
      "  Train results batch 0, loss 0.544592\n",
      "  Train results batch 1, loss 0.379524\n",
      "  Train results batch 2, loss 0.400014\n",
      "  Train results batch 3, loss 0.475495\n",
      "  Train results batch 4, loss 0.416946\n",
      "  Train results batch 5, loss 0.384585\n",
      "  Train results batch 6, loss 0.43549\n",
      "  Train results batch 7, loss 0.35341\n",
      "  Train results batch 8, loss 0.4311\n",
      "  Train results batch 9, loss 0.415991\n",
      "  Train results batch 10, loss 0.335865\n",
      "  Train results batch 11, loss 0.495568\n",
      "  Train results batch 12, loss 0.423765\n",
      "  Train results batch 13, loss 0.412876\n",
      "  Train results batch 14, loss 0.564563\n",
      "  Train results batch 15, loss 0.639854\n",
      "  Train results batch 16, loss 0.345936\n",
      "  Train results batch 17, loss 0.62336\n",
      "  Train results batch 18, loss 0.590548\n",
      "  Train results batch 19, loss 0.395038\n",
      "  Train results batch 20, loss 0.64593\n",
      "  Train results batch 21, loss 0.49745\n",
      "  Train results batch 22, loss 0.43389\n",
      "  Train results batch 23, loss 0.773254\n",
      "  Train results batch 24, loss 0.68614\n",
      "  Train results batch 25, loss 0.344333\n",
      "  Train results batch 26, loss 0.650182\n",
      "  Train results batch 27, loss 0.541339\n",
      "  Train results batch 28, loss 0.400617\n",
      "  Train results batch 29, loss 0.493709\n",
      "  Train results batch 30, loss 0.385434\n",
      "  Train results batch 31, loss 0.425142\n",
      "  Train results batch 32, loss 0.582013\n",
      "  Train results batch 33, loss 0.367218\n",
      "  Train results batch 34, loss 0.360937\n",
      "  Train results batch 35, loss 0.482129\n",
      "  Train results batch 36, loss 0.362854\n",
      "  Train results batch 37, loss 0.464366\n",
      "  Train results batch 38, loss 0.428319\n",
      "  Train results batch 39, loss 0.396171\n",
      "  Train results batch 40, loss 0.648746\n",
      "  Train results batch 41, loss 0.36345\n",
      "  Train results batch 42, loss 0.505143\n",
      "  Train results batch 43, loss 0.704154\n",
      "  Train results batch 44, loss 0.380739\n",
      "  Train results batch 45, loss 0.684524\n",
      "  Train results batch 46, loss 0.884219\n",
      "  Train results batch 47, loss 0.464963\n",
      "  Train results batch 48, loss 0.409934\n",
      "  Train results batch 49, loss 0.552873\n",
      "  Train results batch 50, loss 0.431964\n",
      "  Train results batch 51, loss 0.411451\n",
      "  Train results batch 52, loss 0.412519\n",
      "  Train results batch 53, loss 0.360873\n",
      "  Train results batch 54, loss 0.43386\n",
      "  Train results batch 55, loss 0.395561\n",
      "  Train results batch 56, loss 0.318195\n",
      "  Train results batch 57, loss 0.404171\n",
      "Training results epoch 16, loss 0.000919331387314\n",
      "  Dev results batch 0, loss 0.436823\n",
      "  Dev results batch 1, loss 0.426111\n",
      "  Dev results batch 2, loss 0.430901\n",
      "  Dev results batch 3, loss 0.417163\n",
      "  Dev results batch 4, loss 0.471569\n",
      "  Dev results batch 5, loss 0.409545\n",
      "  Dev results batch 6, loss 0.430006\n",
      "  Dev results batch 7, loss 0.424314\n",
      "  Dev results batch 8, loss 0.451217\n",
      "  Dev results batch 9, loss 0.462839\n",
      "  Dev results batch 10, loss 0.446813\n",
      "  Dev results batch 11, loss 0.399416\n",
      "  Dev results batch 12, loss 0.39294\n",
      "  Dev results batch 13, loss 0.446185\n",
      "  Dev results batch 14, loss 0.486003\n",
      "  Dev results batch 15, loss 0.487958\n",
      "  Dev results batch 16, loss 0.479441\n",
      "  Dev results batch 17, loss 0.460225\n",
      "  Dev results batch 18, loss 0.381457\n",
      "Dev results epoch 16, loss 0.000835011096458\n",
      "  Train results batch 0, loss 0.460947\n",
      "  Train results batch 1, loss 0.325819\n",
      "  Train results batch 2, loss 0.482261\n",
      "  Train results batch 3, loss 0.468039\n",
      "  Train results batch 4, loss 0.360361\n",
      "  Train results batch 5, loss 0.43438\n",
      "  Train results batch 6, loss 0.500589\n",
      "  Train results batch 7, loss 0.395716\n",
      "  Train results batch 8, loss 0.569193\n",
      "  Train results batch 9, loss 0.581709\n",
      "  Train results batch 10, loss 0.394625\n",
      "  Train results batch 11, loss 0.434498\n",
      "  Train results batch 12, loss 0.488236\n",
      "  Train results batch 13, loss 0.410335\n",
      "  Train results batch 14, loss 0.653311\n",
      "  Train results batch 15, loss 0.544243\n",
      "  Train results batch 16, loss 0.518792\n",
      "  Train results batch 17, loss 0.690352\n",
      "  Train results batch 18, loss 0.509266\n",
      "  Train results batch 19, loss 0.482547\n",
      "  Train results batch 20, loss 0.80879\n",
      "  Train results batch 21, loss 0.449017\n",
      "  Train results batch 22, loss 0.493785\n",
      "  Train results batch 23, loss 0.590401\n",
      "  Train results batch 24, loss 0.422655\n",
      "  Train results batch 25, loss 0.499827\n",
      "  Train results batch 26, loss 0.576663\n",
      "  Train results batch 27, loss 0.384787\n",
      "  Train results batch 28, loss 0.443212\n",
      "  Train results batch 29, loss 0.513555\n",
      "  Train results batch 30, loss 0.412611\n",
      "  Train results batch 31, loss 0.367906\n",
      "  Train results batch 32, loss 0.464921\n",
      "  Train results batch 33, loss 0.448043\n",
      "  Train results batch 34, loss 0.361233\n",
      "  Train results batch 35, loss 0.511985\n",
      "  Train results batch 36, loss 0.448676\n",
      "  Train results batch 37, loss 0.432482\n",
      "  Train results batch 38, loss 0.428052\n",
      "  Train results batch 39, loss 0.449283\n",
      "  Train results batch 40, loss 0.379782\n",
      "  Train results batch 41, loss 0.51808\n",
      "  Train results batch 42, loss 0.552491\n",
      "  Train results batch 43, loss 0.369247\n",
      "  Train results batch 44, loss 0.415311\n",
      "  Train results batch 45, loss 0.421831\n",
      "  Train results batch 46, loss 0.409427\n",
      "  Train results batch 47, loss 0.418868\n",
      "  Train results batch 48, loss 0.346013\n",
      "  Train results batch 49, loss 0.392455\n",
      "  Train results batch 50, loss 0.410055\n",
      "  Train results batch 51, loss 0.371493\n",
      "  Train results batch 52, loss 0.463657\n",
      "  Train results batch 53, loss 0.416635\n",
      "  Train results batch 54, loss 0.392955\n",
      "  Train results batch 55, loss 0.388334\n",
      "  Train results batch 56, loss 0.493665\n",
      "  Train results batch 57, loss 0.479729\n",
      "Training results epoch 17, loss 0.000895970540936\n",
      "  Dev results batch 0, loss 0.399879\n",
      "  Dev results batch 1, loss 0.403911\n",
      "  Dev results batch 2, loss 0.364878\n",
      "  Dev results batch 3, loss 0.372137\n",
      "  Dev results batch 4, loss 0.38884\n",
      "  Dev results batch 5, loss 0.368173\n",
      "  Dev results batch 6, loss 0.340841\n",
      "  Dev results batch 7, loss 0.362799\n",
      "  Dev results batch 8, loss 0.395452\n",
      "  Dev results batch 9, loss 0.382883\n",
      "  Dev results batch 10, loss 0.400457\n",
      "  Dev results batch 11, loss 0.389177\n",
      "  Dev results batch 12, loss 0.332417\n",
      "  Dev results batch 13, loss 0.390306\n",
      "  Dev results batch 14, loss 0.436032\n",
      "  Dev results batch 15, loss 0.38603\n",
      "  Dev results batch 16, loss 0.404807\n",
      "  Dev results batch 17, loss 0.386524\n",
      "  Dev results batch 18, loss 0.351524\n",
      "Dev results epoch 17, loss 0.000726505763443\n",
      "  Train results batch 0, loss 0.404626\n",
      "  Train results batch 1, loss 0.37151\n",
      "  Train results batch 2, loss 0.349656\n",
      "  Train results batch 3, loss 0.357193\n",
      "  Train results batch 4, loss 0.355726\n",
      "  Train results batch 5, loss 0.455443\n",
      "  Train results batch 6, loss 0.363354\n",
      "  Train results batch 7, loss 0.360332\n",
      "  Train results batch 8, loss 0.430635\n",
      "  Train results batch 9, loss 0.343869\n",
      "  Train results batch 10, loss 0.430041\n",
      "  Train results batch 11, loss 0.454907\n",
      "  Train results batch 12, loss 0.415038\n",
      "  Train results batch 13, loss 0.418167\n",
      "  Train results batch 14, loss 0.408986\n",
      "  Train results batch 15, loss 0.393779\n",
      "  Train results batch 16, loss 0.395609\n",
      "  Train results batch 17, loss 0.374995\n",
      "  Train results batch 18, loss 0.383996\n",
      "  Train results batch 19, loss 0.443188\n",
      "  Train results batch 20, loss 0.314499\n",
      "  Train results batch 21, loss 0.552787\n",
      "  Train results batch 22, loss 0.447336\n",
      "  Train results batch 23, loss 0.503414\n",
      "  Train results batch 24, loss 0.410892\n",
      "  Train results batch 25, loss 0.447715\n",
      "  Train results batch 26, loss 0.375647\n",
      "  Train results batch 27, loss 0.521148\n",
      "  Train results batch 28, loss 0.426328\n",
      "  Train results batch 29, loss 0.431784\n",
      "  Train results batch 30, loss 0.48145\n",
      "  Train results batch 31, loss 0.530379\n",
      "  Train results batch 32, loss 0.483997\n",
      "  Train results batch 33, loss 0.477267\n",
      "  Train results batch 34, loss 0.42705\n",
      "  Train results batch 35, loss 0.357633\n",
      "  Train results batch 36, loss 0.430277\n",
      "  Train results batch 37, loss 0.469373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train results batch 38, loss 0.443852\n",
      "  Train results batch 39, loss 0.426562\n",
      "  Train results batch 40, loss 0.464662\n",
      "  Train results batch 41, loss 0.435727\n",
      "  Train results batch 42, loss 0.666667\n",
      "  Train results batch 43, loss 0.577548\n",
      "  Train results batch 44, loss 0.547577\n",
      "  Train results batch 45, loss 0.602061\n",
      "  Train results batch 46, loss 0.838545\n",
      "  Train results batch 47, loss 0.461047\n",
      "  Train results batch 48, loss 0.842091\n",
      "  Train results batch 49, loss 0.501634\n",
      "  Train results batch 50, loss 0.812919\n",
      "  Train results batch 51, loss 1.16382\n",
      "  Train results batch 52, loss 0.512962\n",
      "  Train results batch 53, loss 0.548954\n",
      "  Train results batch 54, loss 1.02244\n",
      "  Train results batch 55, loss 0.549562\n",
      "  Train results batch 56, loss 0.477284\n",
      "  Train results batch 57, loss 0.939979\n",
      "Training results epoch 18, loss 0.000962127279636\n",
      "  Dev results batch 0, loss 0.532525\n",
      "  Dev results batch 1, loss 0.630548\n",
      "  Dev results batch 2, loss 0.532717\n",
      "  Dev results batch 3, loss 0.53229\n",
      "  Dev results batch 4, loss 0.59088\n",
      "  Dev results batch 5, loss 0.506571\n",
      "  Dev results batch 6, loss 0.523224\n",
      "  Dev results batch 7, loss 0.528108\n",
      "  Dev results batch 8, loss 0.587785\n",
      "  Dev results batch 9, loss 0.559473\n",
      "  Dev results batch 10, loss 0.591965\n",
      "  Dev results batch 11, loss 0.587151\n",
      "  Dev results batch 12, loss 0.5047\n",
      "  Dev results batch 13, loss 0.595222\n",
      "  Dev results batch 14, loss 0.640518\n",
      "  Dev results batch 15, loss 0.549336\n",
      "  Dev results batch 16, loss 0.566258\n",
      "  Dev results batch 17, loss 0.624403\n",
      "  Dev results batch 18, loss 0.510611\n",
      "Dev results epoch 18, loss 0.00107060597936\n",
      "  Train results batch 0, loss 0.526874\n",
      "  Train results batch 1, loss 0.442079\n",
      "  Train results batch 2, loss 0.667379\n",
      "  Train results batch 3, loss 0.495186\n",
      "  Train results batch 4, loss 0.455977\n",
      "  Train results batch 5, loss 0.521084\n",
      "  Train results batch 6, loss 0.439855\n",
      "  Train results batch 7, loss 0.412144\n",
      "  Train results batch 8, loss 0.496538\n",
      "  Train results batch 9, loss 0.407334\n",
      "  Train results batch 10, loss 0.40727\n",
      "  Train results batch 11, loss 0.348227\n",
      "  Train results batch 12, loss 0.347149\n",
      "  Train results batch 13, loss 0.407546\n",
      "  Train results batch 14, loss 0.388739\n",
      "  Train results batch 15, loss 0.405112\n",
      "  Train results batch 16, loss 0.449319\n",
      "  Train results batch 17, loss 0.397124\n",
      "  Train results batch 18, loss 0.35988\n",
      "  Train results batch 19, loss 0.34505\n",
      "  Train results batch 20, loss 0.383407\n",
      "  Train results batch 21, loss 0.384926\n",
      "  Train results batch 22, loss 0.417152\n",
      "  Train results batch 23, loss 0.447603\n",
      "  Train results batch 24, loss 0.363978\n",
      "  Train results batch 25, loss 0.358441\n",
      "  Train results batch 26, loss 0.408985\n",
      "  Train results batch 27, loss 0.384234\n",
      "  Train results batch 28, loss 0.361943\n",
      "  Train results batch 29, loss 0.373219\n",
      "  Train results batch 30, loss 0.369526\n",
      "  Train results batch 31, loss 0.342792\n",
      "  Train results batch 32, loss 0.36057\n",
      "  Train results batch 33, loss 0.345703\n",
      "  Train results batch 34, loss 0.401824\n",
      "  Train results batch 35, loss 0.429114\n",
      "  Train results batch 36, loss 0.373033\n",
      "  Train results batch 37, loss 0.359893\n",
      "  Train results batch 38, loss 0.392061\n",
      "  Train results batch 39, loss 0.376119\n",
      "  Train results batch 40, loss 0.398775\n",
      "  Train results batch 41, loss 0.368983\n",
      "  Train results batch 42, loss 0.389191\n",
      "  Train results batch 43, loss 0.428805\n",
      "  Train results batch 44, loss 0.36505\n",
      "  Train results batch 45, loss 0.393356\n",
      "  Train results batch 46, loss 0.421539\n",
      "  Train results batch 47, loss 0.369328\n",
      "  Train results batch 48, loss 0.497882\n",
      "  Train results batch 49, loss 0.383044\n",
      "  Train results batch 50, loss 0.403179\n",
      "  Train results batch 51, loss 0.49792\n",
      "  Train results batch 52, loss 0.505815\n",
      "  Train results batch 53, loss 0.407588\n",
      "  Train results batch 54, loss 0.429231\n",
      "  Train results batch 55, loss 0.393585\n",
      "  Train results batch 56, loss 0.37095\n",
      "  Train results batch 57, loss 0.400709\n",
      "Training results epoch 19, loss 0.000793410946087\n",
      "  Dev results batch 0, loss 0.386979\n",
      "  Dev results batch 1, loss 0.379236\n",
      "  Dev results batch 2, loss 0.363375\n",
      "  Dev results batch 3, loss 0.357552\n",
      "  Dev results batch 4, loss 0.404879\n",
      "  Dev results batch 5, loss 0.343824\n",
      "  Dev results batch 6, loss 0.381412\n",
      "  Dev results batch 7, loss 0.361957\n",
      "  Dev results batch 8, loss 0.397709\n",
      "  Dev results batch 9, loss 0.404004\n",
      "  Dev results batch 10, loss 0.401193\n",
      "  Dev results batch 11, loss 0.357059\n",
      "  Dev results batch 12, loss 0.333319\n",
      "  Dev results batch 13, loss 0.401894\n",
      "  Dev results batch 14, loss 0.443455\n",
      "  Dev results batch 15, loss 0.412234\n",
      "  Dev results batch 16, loss 0.416301\n",
      "  Dev results batch 17, loss 0.389442\n",
      "  Dev results batch 18, loss 0.33337\n",
      "Dev results epoch 19, loss 0.000727720014898\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle (start_indices)\n",
    "train_indices = start_indices[0:int(train_size)]\n",
    "dev_indices= start_indices[int(train_size):int(train_size+dev_size)]\n",
    "test_indices = start_indices[int(train_size+dev_size):int(train_size+dev_size+test_size)]\n",
    "\n",
    "epoch_count = 20\n",
    "\n",
    "loss_results = np.zeros((epoch_count,2))\n",
    "\n",
    "def get_dev_loss():\n",
    "    epoch_dev_loss = 0.0\n",
    "    for devi in range(dev_batch_count):\n",
    "        batch_inputs,batch_targets = get_batch(devi, dev_indices)\n",
    "\n",
    "        batch_dev_loss = sess.run(loss,feed_dict={inputs:batch_inputs,targets:batch_targets})\n",
    "        print(\"  Dev results batch %d, loss %s\" %(  devi, str(batch_dev_loss)))  \n",
    "\n",
    "        epoch_dev_loss += batch_dev_loss\n",
    "        #sys.stdout.write('.')\n",
    "        #sys.stdout.flush()\n",
    "    return epoch_dev_loss / dev_size\n",
    "\n",
    "def generate_graph():\n",
    "    graph_size = 200\n",
    "    prime_size = 20\n",
    "    \n",
    "    prime_signal_start_i = 0\n",
    "    \n",
    "    tmp_signal = np.zeros((graph_size,1))\n",
    "    tmp_signal[0:prime_size,0] = signal_amp[prime_signal_start_i:(prime_signal_start_i+prime_size)]\n",
    "    \n",
    "    tmp_batch = np.zeros((batch_size,sequence_length,1))\n",
    "    print(tmp_batch.shape)\n",
    "    print(inputs.shape)\n",
    "    p_i_e = prime_size\n",
    "    p_i_s = p_i_e - sequence_length\n",
    "    \n",
    "    for end in range(prime_size, graph_size):\n",
    "        #end = prime_size\n",
    "        tmp_batch[0,:,0] = tmp_signal.take(range((end-sequence_length),end), mode='wrap')\n",
    "        _predictions = sess.run([predictions[0,0]], feed_dict={learning_rate: 0.02, inputs: tmp_batch})\n",
    "        #print(_predictions)\n",
    "        tmp_signal[end,0] = _predictions[0]\n",
    "        sys.stdout.write('.')\n",
    "        sys.stdout.flush()\n",
    "    print(\"\")\n",
    "    \n",
    "    plotly.offline.iplot({\n",
    "        \"data\": [Scatter(y=tmp_signal[:,0])],\n",
    "        \"layout\": Layout(title=\"\")\n",
    "    })\n",
    "\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    epoch_dev_loss = get_dev_loss()    \n",
    "\n",
    "    #print(\"\")            \n",
    "    print(\"Dev results epoch start, loss %s\" %(  str(epoch_dev_loss),))  \n",
    "\n",
    "    for epoch in range(0,epoch_count):\n",
    "        np.random.shuffle (train_indices)\n",
    "        epoch_train_loss = 0.0\n",
    "        for ti in range(train_batch_count):\n",
    "            batch_inputs,batch_targets = get_batch(ti, train_indices)\n",
    "\n",
    "            batch_train_loss, _ = sess.run([loss, opt], feed_dict={learning_rate: 0.02, inputs: batch_inputs, targets: batch_targets})\n",
    "            print(\"  Train results batch %d, loss %s\" %(  ti, str(batch_train_loss)))  \n",
    "            epoch_train_loss += batch_train_loss\n",
    "            #sys.stdout.write('.')\n",
    "            #sys.stdout.flush()\n",
    "        #print(\"\")\n",
    "        epoch_train_loss = epoch_train_loss / train_size\n",
    "        print(\"Training results epoch %d, loss %s\" %( epoch, str(epoch_train_loss)))\n",
    "        epoch_dev_loss = get_dev_loss()    \n",
    "        #print(\"\")            \n",
    "        print(\"Dev results epoch %d, loss %s\" %( epoch, str(epoch_dev_loss)))  \n",
    "        loss_results[epoch,0] = epoch_train_loss\n",
    "        loss_results[epoch,1] = epoch_dev_loss\n",
    "        ti += 1\n",
    "        #generate_graph()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "scatter",
         "y": [
          2.607230132573159e-06,
          1.1556854446689457e-06,
          8.545980664719434e-07,
          7.429411213903806e-07,
          6.575158147895133e-07,
          6.143927310586994e-07,
          5.231702305173119e-07,
          4.944786284203599e-07,
          4.399237643014005e-07,
          4.846780341690525e-07,
          3.632786044193914e-07,
          4.4268466333076315e-07,
          4.6047056874743573e-07,
          2.244911930524947e-06,
          1.4703363681860026e-05,
          7.444966801395812e-05,
          4.461573258795712e-06,
          2.3905960308142513e-07,
          3.451094580668116e-07,
          5.191599826591204e-06,
          7.06698929968427e-05,
          1.218874917661452e-06,
          8.517643758455252e-06,
          2.274419746590161e-05,
          5.800174328129599e-05,
          7.901080088821491e-05,
          1.5771402782339164e-05,
          5.741641217814918e-06
         ]
        },
        {
         "type": "scatter",
         "y": [
          1.5554299689672714e-06,
          9.003760104698544e-07,
          7.720906961985609e-07,
          8.056022491877036e-07,
          7.679703466770246e-07,
          4.992880173438121e-07,
          5.360317475180441e-07,
          5.390737140246332e-07,
          4.843227207942804e-07,
          3.14513154405534e-07,
          6.68388608712e-07,
          4.6991961415904064e-07,
          5.044216501269859e-07,
          2.194346093847439e-05,
          3.386400652364447e-07,
          4.065463155141069e-05,
          2.3029761905581935e-07,
          3.2805666808894473e-07,
          2.5714920301895466e-07,
          8.694693783296457e-05,
          2.039305988648105e-06,
          1.053851740349922e-06,
          3.903783281258318e-05,
          9.725807893442098e-06,
          5.963183764175486e-05,
          3.776913203895002e-05,
          2.2087188840168997e-05,
          6.728979974011005e-06
         ]
        }
       ],
       "layout": {
        "title": ""
       }
      },
      "text/html": [
       "<div id=\"9c969e55-988e-4fda-9335-da1f1cca6654\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9c969e55-988e-4fda-9335-da1f1cca6654\", [{\"type\": \"scatter\", \"y\": [2.607230132573159e-06, 1.1556854446689457e-06, 8.545980664719434e-07, 7.429411213903806e-07, 6.575158147895133e-07, 6.143927310586994e-07, 5.231702305173119e-07, 4.944786284203599e-07, 4.399237643014005e-07, 4.846780341690525e-07, 3.632786044193914e-07, 4.4268466333076315e-07, 4.6047056874743573e-07, 2.244911930524947e-06, 1.4703363681860026e-05, 7.444966801395812e-05, 4.461573258795712e-06, 2.3905960308142513e-07, 3.451094580668116e-07, 5.191599826591204e-06, 7.06698929968427e-05, 1.218874917661452e-06, 8.517643758455252e-06, 2.274419746590161e-05, 5.800174328129599e-05, 7.901080088821491e-05, 1.5771402782339164e-05, 5.741641217814918e-06]}, {\"type\": \"scatter\", \"y\": [1.5554299689672714e-06, 9.003760104698544e-07, 7.720906961985609e-07, 8.056022491877036e-07, 7.679703466770246e-07, 4.992880173438121e-07, 5.360317475180441e-07, 5.390737140246332e-07, 4.843227207942804e-07, 3.14513154405534e-07, 6.68388608712e-07, 4.6991961415904064e-07, 5.044216501269859e-07, 2.194346093847439e-05, 3.386400652364447e-07, 4.065463155141069e-05, 2.3029761905581935e-07, 3.2805666808894473e-07, 2.5714920301895466e-07, 8.694693783296457e-05, 2.039305988648105e-06, 1.053851740349922e-06, 3.903783281258318e-05, 9.725807893442098e-06, 5.963183764175486e-05, 3.776913203895002e-05, 2.2087188840168997e-05, 6.728979974011005e-06]}], {\"title\": \"\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"9c969e55-988e-4fda-9335-da1f1cca6654\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9c969e55-988e-4fda-9335-da1f1cca6654\", [{\"type\": \"scatter\", \"y\": [2.607230132573159e-06, 1.1556854446689457e-06, 8.545980664719434e-07, 7.429411213903806e-07, 6.575158147895133e-07, 6.143927310586994e-07, 5.231702305173119e-07, 4.944786284203599e-07, 4.399237643014005e-07, 4.846780341690525e-07, 3.632786044193914e-07, 4.4268466333076315e-07, 4.6047056874743573e-07, 2.244911930524947e-06, 1.4703363681860026e-05, 7.444966801395812e-05, 4.461573258795712e-06, 2.3905960308142513e-07, 3.451094580668116e-07, 5.191599826591204e-06, 7.06698929968427e-05, 1.218874917661452e-06, 8.517643758455252e-06, 2.274419746590161e-05, 5.800174328129599e-05, 7.901080088821491e-05, 1.5771402782339164e-05, 5.741641217814918e-06]}, {\"type\": \"scatter\", \"y\": [1.5554299689672714e-06, 9.003760104698544e-07, 7.720906961985609e-07, 8.056022491877036e-07, 7.679703466770246e-07, 4.992880173438121e-07, 5.360317475180441e-07, 5.390737140246332e-07, 4.843227207942804e-07, 3.14513154405534e-07, 6.68388608712e-07, 4.6991961415904064e-07, 5.044216501269859e-07, 2.194346093847439e-05, 3.386400652364447e-07, 4.065463155141069e-05, 2.3029761905581935e-07, 3.2805666808894473e-07, 2.5714920301895466e-07, 8.694693783296457e-05, 2.039305988648105e-06, 1.053851740349922e-06, 3.903783281258318e-05, 9.725807893442098e-06, 5.963183764175486e-05, 3.776913203895002e-05, 2.2087188840168997e-05, 6.728979974011005e-06]}], {\"title\": \"\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "scatter",
         "y": [
          2.607230132573159e-06,
          1.1556854446689457e-06,
          8.545980664719434e-07,
          7.429411213903806e-07,
          6.575158147895133e-07,
          6.143927310586994e-07,
          5.231702305173119e-07,
          4.944786284203599e-07,
          4.399237643014005e-07,
          4.846780341690525e-07,
          3.632786044193914e-07,
          4.4268466333076315e-07,
          4.6047056874743573e-07,
          2.244911930524947e-06,
          1.4703363681860026e-05,
          7.444966801395812e-05,
          4.461573258795712e-06,
          2.3905960308142513e-07,
          3.451094580668116e-07,
          5.191599826591204e-06,
          7.06698929968427e-05,
          1.218874917661452e-06,
          8.517643758455252e-06,
          2.274419746590161e-05,
          5.800174328129599e-05,
          7.901080088821491e-05,
          1.5771402782339164e-05,
          5.741641217814918e-06
         ]
        },
        {
         "type": "scatter",
         "y": [
          1.5554299689672714e-06,
          9.003760104698544e-07,
          7.720906961985609e-07,
          8.056022491877036e-07,
          7.679703466770246e-07,
          4.992880173438121e-07,
          5.360317475180441e-07,
          5.390737140246332e-07,
          4.843227207942804e-07,
          3.14513154405534e-07,
          6.68388608712e-07,
          4.6991961415904064e-07,
          5.044216501269859e-07,
          2.194346093847439e-05,
          3.386400652364447e-07,
          4.065463155141069e-05,
          2.3029761905581935e-07,
          3.2805666808894473e-07,
          2.5714920301895466e-07,
          8.694693783296457e-05,
          2.039305988648105e-06,
          1.053851740349922e-06,
          3.903783281258318e-05,
          9.725807893442098e-06,
          5.963183764175486e-05,
          3.776913203895002e-05,
          2.2087188840168997e-05,
          6.728979974011005e-06
         ]
        }
       ],
       "layout": {
        "title": ""
       }
      },
      "text/html": [
       "<div id=\"6d879fe0-add5-4b6e-8ccd-9e6a6479f5aa\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"6d879fe0-add5-4b6e-8ccd-9e6a6479f5aa\", [{\"type\": \"scatter\", \"y\": [2.607230132573159e-06, 1.1556854446689457e-06, 8.545980664719434e-07, 7.429411213903806e-07, 6.575158147895133e-07, 6.143927310586994e-07, 5.231702305173119e-07, 4.944786284203599e-07, 4.399237643014005e-07, 4.846780341690525e-07, 3.632786044193914e-07, 4.4268466333076315e-07, 4.6047056874743573e-07, 2.244911930524947e-06, 1.4703363681860026e-05, 7.444966801395812e-05, 4.461573258795712e-06, 2.3905960308142513e-07, 3.451094580668116e-07, 5.191599826591204e-06, 7.06698929968427e-05, 1.218874917661452e-06, 8.517643758455252e-06, 2.274419746590161e-05, 5.800174328129599e-05, 7.901080088821491e-05, 1.5771402782339164e-05, 5.741641217814918e-06]}, {\"type\": \"scatter\", \"y\": [1.5554299689672714e-06, 9.003760104698544e-07, 7.720906961985609e-07, 8.056022491877036e-07, 7.679703466770246e-07, 4.992880173438121e-07, 5.360317475180441e-07, 5.390737140246332e-07, 4.843227207942804e-07, 3.14513154405534e-07, 6.68388608712e-07, 4.6991961415904064e-07, 5.044216501269859e-07, 2.194346093847439e-05, 3.386400652364447e-07, 4.065463155141069e-05, 2.3029761905581935e-07, 3.2805666808894473e-07, 2.5714920301895466e-07, 8.694693783296457e-05, 2.039305988648105e-06, 1.053851740349922e-06, 3.903783281258318e-05, 9.725807893442098e-06, 5.963183764175486e-05, 3.776913203895002e-05, 2.2087188840168997e-05, 6.728979974011005e-06]}], {\"title\": \"\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"6d879fe0-add5-4b6e-8ccd-9e6a6479f5aa\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"6d879fe0-add5-4b6e-8ccd-9e6a6479f5aa\", [{\"type\": \"scatter\", \"y\": [2.607230132573159e-06, 1.1556854446689457e-06, 8.545980664719434e-07, 7.429411213903806e-07, 6.575158147895133e-07, 6.143927310586994e-07, 5.231702305173119e-07, 4.944786284203599e-07, 4.399237643014005e-07, 4.846780341690525e-07, 3.632786044193914e-07, 4.4268466333076315e-07, 4.6047056874743573e-07, 2.244911930524947e-06, 1.4703363681860026e-05, 7.444966801395812e-05, 4.461573258795712e-06, 2.3905960308142513e-07, 3.451094580668116e-07, 5.191599826591204e-06, 7.06698929968427e-05, 1.218874917661452e-06, 8.517643758455252e-06, 2.274419746590161e-05, 5.800174328129599e-05, 7.901080088821491e-05, 1.5771402782339164e-05, 5.741641217814918e-06]}, {\"type\": \"scatter\", \"y\": [1.5554299689672714e-06, 9.003760104698544e-07, 7.720906961985609e-07, 8.056022491877036e-07, 7.679703466770246e-07, 4.992880173438121e-07, 5.360317475180441e-07, 5.390737140246332e-07, 4.843227207942804e-07, 3.14513154405534e-07, 6.68388608712e-07, 4.6991961415904064e-07, 5.044216501269859e-07, 2.194346093847439e-05, 3.386400652364447e-07, 4.065463155141069e-05, 2.3029761905581935e-07, 3.2805666808894473e-07, 2.5714920301895466e-07, 8.694693783296457e-05, 2.039305988648105e-06, 1.053851740349922e-06, 3.903783281258318e-05, 9.725807893442098e-06, 5.963183764175486e-05, 3.776913203895002e-05, 2.2087188840168997e-05, 6.728979974011005e-06]}], {\"title\": \"\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotly.offline.iplot({\n",
    "    \"data\": [Scatter(y=loss_results[2:,0]),Scatter(y=loss_results[2:,1])],\n",
    "    \"layout\": Layout(title=\"\")\n",
    "})\n",
    "plotly.offline.iplot\n",
    "plotly.offline.iplot({\n",
    "    \"data\": [Scatter(y=loss_results[2:,0]),Scatter(y=loss_results[2:,1])],\n",
    "    \"layout\": Layout(title=\"\")\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "scatter",
         "y": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.06279177368663903,
          0.1253357270201141,
          0.18738501772008576,
          0.24869475579170244,
          0.3090229700331771,
          0.36813156302376016,
          0.4257872508230578,
          0.4817624836729815,
          0.5358363440685963,
          0.5877954186534478,
          0.6374346404982605,
          0.6845580984387827,
          0.7289798102785665,
          0.7705244568050786,
          0.8090280737222055,
          0.844338698768296,
          0.8763169714657656,
          0.9048366831352256,
          0.9297852750033988
         ]
        }
       ],
       "layout": {
        "title": ""
       }
      },
      "text/html": [
       "<div id=\"8792e855-5021-419b-bf43-9d282694704f\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"8792e855-5021-419b-bf43-9d282694704f\", [{\"type\": \"scatter\", \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06279177368663903, 0.1253357270201141, 0.18738501772008576, 0.24869475579170244, 0.3090229700331771, 0.36813156302376016, 0.4257872508230578, 0.4817624836729815, 0.5358363440685963, 0.5877954186534478, 0.6374346404982605, 0.6845580984387827, 0.7289798102785665, 0.7705244568050786, 0.8090280737222055, 0.844338698768296, 0.8763169714657656, 0.9048366831352256, 0.9297852750033988]}], {\"title\": \"\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"8792e855-5021-419b-bf43-9d282694704f\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"8792e855-5021-419b-bf43-9d282694704f\", [{\"type\": \"scatter\", \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06279177368663903, 0.1253357270201141, 0.18738501772008576, 0.24869475579170244, 0.3090229700331771, 0.36813156302376016, 0.4257872508230578, 0.4817624836729815, 0.5358363440685963, 0.5877954186534478, 0.6374346404982605, 0.6845580984387827, 0.7289798102785665, 0.7705244568050786, 0.8090280737222055, 0.844338698768296, 0.8763169714657656, 0.9048366831352256, 0.9297852750033988]}], {\"title\": \"\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "generate_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
